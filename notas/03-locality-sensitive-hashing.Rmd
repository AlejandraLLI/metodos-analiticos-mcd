# Similitud: Locality Sensitive Hashing

```{r, echo=FALSE, message=FALSE}
library(tidyverse)
theme_set(theme_bw())
cb_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```


En esta sección explicamos como hacer la búsqueda de pares
similares, y después mostraremos cómo aplicar
estas técnicas para otras medidas de distancia  (como distancia
euclideana y coseno). 

La técnica de LSH (Locality Sensitive
Hashing) consiste en poner en cubetas a elementos que tengan 
hashes similares. Si diseñamos correctamente el método, entonces
no es necesario hacer todas las comparaciones entre los pares,
y basta examinar los elementos que compartan cubeta con otros elementos
(eliminando la mayor parte de las cubetas que tendrán solo un elemento).


## Análisis de la técnica de bandas 

En la sección anterior dimos la primera idea como usar
la *técnica de bandas* con minhashes para encontrar documentos de similitud alta, con distintos umbrales de similitud alta. Aquí describimos un análisis
más detallado de la técnica

```{block2 , type='resumen'}
Supongamos que tenemos un total de $k$ minhashes, que dividimos
en $b$ bandas de tamaño $r$, de modo que $k=br$. 

- Decimos que un par de documentos *coinciden* en una banda de $r$ hashes
si coinciden en todos los hashes de esa banda.

- Un par de documentos es un **par candidato** si 
al menos coinciden en una banda (es decir, en al menos dentro
de una banda todos los minhashes coinciden).

```

Ahora vamos a calcular la probabilidad de que un par de documentos
con similitud $s$ sean un par candidato:

1. La probabilidad de que estos dos documentos coincidan en un hash
particular es $s$, la similitud de Jaccard.
2. La probabiliad de que todos los hashes de una banda coincidan es
$s^r$, pues seleccionamos los hashes independientemente. 
3. Así que la probabilidad de que los documentos no coincidan en una banda
particular es: $1-s^r$

4. Esto implica que la probabilidad de que los documentos no coincidan en ninguna banda es $(1-s^r)^b$.
5. Finalmente, la probabilidad de que estos dos documentos sean un par candidato es $1-(1-s^r)^b$, que es la probabilidad de que coincidan en al menos una banda.

```{block2, type="resumen"}
Si la similitud de Jaccard de dos documentos es $s$, la probabilidad
de que sean un par candidato es igual a $$1-(1-s^r)^b.$$
```



### Ejemplo {-}
Supongamos que tenemos $8$ minhashes, y que nos
interesa encontrar documentos con similitud mayor a $0.7$. 
Tenemos las siguientes posiblidades.

```{r, fig.width=4, fig.asp=0.6, echo = TRUE}
# Función para graficar curvas
graficar_curvas <- function(df_br, colour = TRUE){
  # extrae tamaño de bandas
  r <- df_br$r 
  # extrae no. de bandas 
  b <- df_br$b
  
  # Para cada pareja (r,b) calcula la prob de que un par sea candidato para
  # distintos valores de similitud de jaccard. 
  curvas_similitud <- data_frame(b = b, r = r) %>%
    group_by(r, b) %>%
    mutate(datos = map2(r, b, function(r, b){
          df_out <- data_frame(s = seq(0, 1, 0.01)) %>% 
            mutate(prob = 1 - (1 - s ^ r) ^b)
          df_out 
          })) %>% unnest
  
  # Grafica las salidas
  graf_salida <- ggplot(curvas_similitud, aes(x = s, y = prob, 
          colour = as.factor(interaction(b,r)))) +
          geom_line(size=1.1) + 
          labs(x = 'similitud', y= 'probablidad de ser candidato',
          colour = 'b.r') 
  if(colour){
    graf_salida + scale_colour_manual(values = cb_palette)
  }
  graf_salida
}
```

```{r, fig.width=4, fig.asp=0.6}
# Define posibles tamaños de banda.
r <- c(1,2,4,8)

# Define no de bandas para cada tamaño.  
df_br <- data_frame(r = r, b = rev(r))

# Grafica las curvas y el umbral para la 
# probabilidad de par candidato.
graficar_curvas(df_br) + geom_vline(xintercept = 0.7)
```

- Con la configuración $b=1, r=8$ (línea morada, un solo grupo de 8 hashes) es posible
que no capturemos muchos pares de la similitud que nos interesa (mayor a $0.7$) ya que hay pares con similitud, por ejemplo, de 0.75 pero con probabilidad de ser candidato de 0.18 aproximadamente.
- Con $b=8, r=1$ (línea roja, al menos un hash de los $8$), dejamos pasar 
demasiados falsos positivos, que después vamos a tener que filtrar. Por ejemplo hay pares con simitud de 0.25, pero probabilidad de ser candidato de 0.9. 
- Los otros dos casos son mejores para nuestro propósito. $b=4$ y $r=2$ (línea verde) produce falsos negativos que hay que filtrar, y para $b=2$ y $r=4$ hay una probabilidad de alrededor de $50\%$
de que no capturemos pares con similitud cercana a $0.7.$


```{block2, type="resumen"}
Siempre hay un  trade-off entre exigir similitud alta y la probabilidad de ser candidato: 
  
  * Pocas bandas con muchos hashes -> no capturo pares de interés. 

  * Muchas bandas con pocos hashes -> tengo muchos falsos positivos. 

En la mayoría de los casos, voy a tener que hacer un filtrado de los falsos positivos. 
```


Generalmente quisiéramos obtener algo más cercano a una función escalón.
Podemos acercarnos si incrementamos el número total de hashes.

```{r, fig.width=4, fig.asp=0.6}
# Tamaño de las bandas
r <- c(4, 5, 8, 10, 20)

# No. bandas= total hashes/tamaño
b <- 80/r

# Grafica las curvas
graficar_curvas(data_frame(b, r)) + geom_vline(xintercept = 0.7) 
```

---

**Observación**: Podemos despejar la similitud de Jaccard de la fórmula anterior
$$
s = \left (1-\left (1-p\right )^{1/b} \right )^{1/r} = \left (1-\left (1-p\right )^{1/b} \right )^{b/h}
$$

Notar que la curva alcanza probabilidad $1/2$ cuando la similitud
es
$$s = \left (1-\left (0.5\right )^{1/b} \right )^{1/r}=\left (1-\left (0.5\right )^{1/b} \right )^{b/h}=.$$
con $h$ el número total de hashes. Podemos usar esta fórmula para escoger valores de $b$ y $r$ apropiados,
dependiendo de qué similitud nos interesa capturar (quizá moviendo un poco
hacia abajo si queremos tener menos falsos negativos).
```{r}
# Función para calcular la probabilidad 0.5
lsh_half <- function(h, b){
    # h es el número total de funciones hash
    # b es el número total de bandas. 
   (1 - (0.5) ^ ( 1/b))^(b/h)
}

# Verifica que la similitud de Jaccard para 80 hashes y 16 bandas
lsh_half(80, 16)
```

En [@mmd], se utiliza la aproximación (según la referencia, aproxima el
punto de máxima pendiente):
```{r}
textreuse::lsh_threshold
```

Que está también implementada en el paquete textreuse [@R-textreuse].

```{r}
textreuse::lsh_threshold(80, 16)
```
### Ejemplo {-}

Supongamos que nos interesan documentos con similitud mayor a $0.5$.
Intentamos con $50$ o $200$ hashes algunas combinaciones:

```{r, fig.width=5, fig.asp=0.6}
# Función 
params_umbral <- function(num_hashes, umbral_inf = 0.0, umbral_sup = 1.0){
  # selecciona combinaciones con umbral-1/2 en un rango 
  # (umbral_inf, umbral_sup)
  
  # No. de bandas posibles
  b <- seq(1, num_hashes)
  
  # Elimina aquellas bandas que no son divisor del
  # no. de hashes
  b <- b[num_hashes %% b == 0] # solo exactos
  
  # Tamaño de las bandas
  r <- floor(num_hashes %/% b)
  
  # Calcula combinaciones posibles.
  combinaciones_pr <- 
    data_frame(b = b, r = r) %>% # combinaciones (b,r)
    unique() %>% # elimina repetidos
    mutate(s = (1 - (0.5)^(1/b))^(1/r)) %>% # Calcula sim jaccard con p=0.5
    filter(s < umbral_sup, s > umbral_inf) # filtra similitudes en humbral. 
  
  # resultado. 
  combinaciones_pr
}

combinaciones_50 <- params_umbral(50, 0.4, 0.7)

combinaciones_50

graficar_curvas(combinaciones_50) + 
  geom_vline(xintercept = 0.4, lty=2) + 
   geom_vline(xintercept = 0.7, lty=2) 
```

Usando 50 hashes sólo se obtiene una posible curva que garantice similitudes de Jaccard de entre 0.4 y 0.7 con al menos 50% de probabilidad de ser candidato. 


Con $200$ hashes podemos obtener curvas con mayor pendiente:

```{r, fig.width=5, fig.asp=0.6}
combinaciones_200 <- params_umbral(200, 0.2, 0.8)

combinaciones_200
graficar_curvas(combinaciones_200) + 
  geom_vline(xintercept = 0.2, lty=2) + 
   geom_vline(xintercept = 0.8, lty=2) 

```

**Observación**: La decisión de los valores para estos parámetros
debe balancear

1. qué tan importante es tener pares no detectados (falsos
negativos),
y 

2. el cómputo necesario para calcular los hashes y filtrar los
falsos positivos.


La ventaja computacional de LSH proviene
de hacer *trade-offs* de lo que es más importante para nuestro
problema.


## Resumen de LSH basado en minhashing

Resumen de [@mmd]

1. Escogemos un número $k$ de tamaño de tejas, y construimos el
conjunto de tejas de cada documento.
2. Ordenar los pares documento-teja y agrupar por teja.
3. Escoger $n$, el número de minhashes. Aplicamos el algoritmo de la
clase anterior (teja por teja) para calcular las 
firmas minhash de todos los documentos. 
4. Escoger el umbral $s$ de similitud que nos ineresa. Escogemos $b$ y $r$
(número de bandas y de qué tamaño), usando la fórmula de arriba hasta
obtener un valor cercano al umbral. 
Si es importante evitar falsos negativos, escoger valores de $b$ y $r$ que
den un umbral más bajo, si la velocidad es importante entonces escoger
para un umbral más alto y evitar falsos positivos. Mayores valores
de $b$ y $r$ pueden dar mejores resultados, pero también requieren
más cómputo.
5. Construir pares similares usando LSH.
6. Examinar las firmas de cada par candidato y determinar si 
la fracción de coincidencias sobre todos los minhashes es satisfactorio.
Alternativamente (más preciso), calcular directamente la similitud 
de Jaccard a partir de las tejas originales. 


Alternativamente, podemos:

2. Agrupar las tejas de cada documento.
3. Escoger $n$, el número de minhashes. Calcular el minhash de cada
documento aplicando una función hash a las tejas del documento.
Tomar el mínimo. Repetir para cada función hash.

## Ejemplo: artículos de wikipedia

En este ejemplo intentamos encontrar artículos similares de [wikipedia](http://wiki.dbpedia.org/datasets/dbpedia-version-2016-10)
 usando las categorías a las que pertenecen. En lugar de usar tejas,
usaremos categorías a las que pertenecen. Dos artículos tienen similitud alta cuando los conjuntos de categorías a las que pertenecen es similar.
(este el [ejemplo original](https://github.com/elmer-garduno/metodos-analiticos/blob/master/Lecture_2_Similarity_Spark.ipynb)).


Empezamos con una muestra de los datos, vienen en el formato (artículo,categoría):

```{r, engine='bash'}
# muestra los 20 primeros datos del txt
head -20 ../datos/similitud/wiki-100000.txt
```

Leemos y limpiamos los datos:

```{r, message = FALSE}
# Funcíon para limpiar los datos
limpiar <- function(lineas,...){
  df_lista <- str_split(lineas, ' ') %>% # separa elementos con el espacio en blanco
    keep(function(x) x[1] != '#') %>% # elimina lineas que inician con #
    transpose %>% # transpone
    map(function(col) as.character(col)) # convierte a character
  
  df <- data_frame(articulo = df_lista[[1]], # extrae artículos
                   categorias = df_lista[[2]]) # extrae categorías
  df
}

# Lee líneas chunks omitiendo la primera y aplicando a cada chunk la función limpiar
filtrado <- read_lines_chunked('../datos/similitud/wiki-100000.txt',
                    skip = 1, callback = ListCallback$new(limpiar))

# Agrupa por artículo y pone las categorías en una lista. 
articulos_df <- filtrado %>% bind_rows %>%
                group_by(articulo) %>%
                summarise(categorias = list(categorias))

# dimensiones de los datos 
dim(articulos_df)
```

```{r}
# Fija la semilla. 
set.seed(99)
# extrae una muestra de 10 artículos 
muestra <- articulos_df %>% sample_n(10)

# Vemos la estructura de los datos limpios 
muestra

# Examinamos las categorías del tercer artículo
muestra$categorias[[3]]
```

El que algunos artículos tengan pocas categorías está relacionado con la forma en que se hizo el muestreo, i.e. se perdieron muchas perjas (artículo, categoría). 

### Selección de número de hashes y bandas {-}

Ahora supongamos que buscamos artículos con similitud mínima
de $0.4$. Experimentando con valores del total de hashes y el número
de bandas, podemos seleccionar, por ejemplo:

```{r, collapse = TRUE, fig.width=5, fig.asp=0.6}
# No. de bandas
b <- 20

# No. de hashes
num_hashes <- 60

# Cácula la similitud a obtener con estos valores 
lsh_half(num_hashes, b = b) 

# Grafica la curva similitud-probabilidad 
graficar_curvas(data_frame(b = b, r = num_hashes/b)) +
                 geom_vline(xintercept = 0.4) 
```

Va a capturar candidatos con similitud mínima de 0.4 con un 75% de probabilidad. 

### Tejas y cálculo de minhashes {-}

```{r}
# Carga funciones de la clase pasada para calcular similitud de jaccard, 
# firmas por teja, por documento, generar hashes, etc.
source('../scripts/lsh/minhash.R')
```

```{r}
# Fija la semilla 
set.seed(28511)

# Establece el no. de hashes. 
num_hashes <- 60

# Genera funciones hash modulares
hash_f <- map(1:num_hashes, ~ generar_hash_mod())

# Función para separar strings por espacio.
tokenize_sp <- function(x,...) stringr::str_split(x, " ")
```


```{r}
# NOTA: nos podemos ahorrar la siguiente línea si adaptamos crear_tejas_doc

# Extrae artículos
articulos <- articulos_df$articulo

# Pega las categorías de un artículo separandolas por espacio. 
textos <- articulos_df$categorias %>% map_chr( ~ paste(.x, collapse = " ")) 

# Crea tejas: convierte categorías a números
tejas_obj <- crear_tejas_doc(textos, k = 1, tokenize_sp)

# Calcula la matriz de firmas por documento
firmas_wiki <- calc_firmas_hash_doc(tejas_obj, hash_f)
head(firmas_wiki)
```


### Agrupación en cubetas {-}

Ahora calculamos las cubetas y agrupamos. Obtiene 20 cubetas formadas por 3 hashes (cateogorías):

```{r}
# Se cargan las funciones para hacer lsh
source("../scripts/lsh/lsh.R")

# Crea cubetas de tamaño 3.
# Nota: al string de cubetas le aplica una función hash de tipo xxhash64
# para obtener una representación más manejable que: 
# 456|371014616-731771297-1033620278
crear_cubetas <- crear_cubetas_fun(60, 3)
crear_cubetas(firmas_wiki$firma[[1]])

# Calcular no. de documentos por cubeta.
lsh_wiki_1 <- firmas_wiki %>% # (articulos,categorias)
  mutate(cubeta = map(firma, crear_cubetas)) %>%  # Crea cubetas
  select(-firma) %>% # Elimina la firma
  unnest %>% # convierte a dataframe
  group_by(cubeta) %>% # agrupa por cubeta
  summarise(docs = list(doc_id))%>%  # agrupa documentoss en lista
  mutate(n_docs = map(docs, length)) # cuenta no. de documentos.

head(lsh_wiki_1)
```

Filtramos las cubetas que tienen más de un documento:

```{r}
# Calcula el no. de documentos y filtra las cubetas con al menos dos doc. 
lsh_agrupados <- lsh_wiki_1 %>%  
    mutate(n_docs = map_int(docs, length)) %>% 
    filter(n_docs > 1)

# No aprox. de documentos.
# Puede haber un mismo doc en más de una cubeta. 
sum(lsh_agrupados$n_docs)
```

Y ahora examinamos algunos de los clusters que encontramos de 
artículos similares según sus categorías:

```{r}
# Función para imprimir texto y sus categorías
imprimir_texto <- function(indices){
    for(ind in indices){
        print(paste(articulos[ind]))
        print(paste("      ", textos[ind]))
    }
}

# Elementos de la cubeta 3
imprimir_texto(lsh_agrupados$docs[[3]])

```

Todos estos artículos están en la categoría de poker, gameplay y terminology. Veamos otros ejemplos: 
```{r}
imprimir_texto(lsh_agrupados$docs[[8]])
```

Notar que el artículo Coprime sólo tiene una categoría. La similtud entre "Amicable_numbers" y "Coprime" es de 1/3=0.33 que es menor al 0.4 que originalmente queríamos. Es decir, se pueden colar algunos artículos con menor similitud a la deseada pero ya es un número más manejable para examinar incluso de uno por uno. 

```{r}
imprimir_texto(lsh_agrupados$docs[[180]])
```

```{r}
imprimir_texto(lsh_agrupados$docs[[255]])
imprimir_texto(lsh_agrupados$docs[[11455]])
```

**Ejercicio**: explora más los grupos creados por las cubetas. Observa que pares
similares dados pueden ocurrir en más de una cubeta (pares repetidos)


### Creación de pares candidatos y falsos positivos {-}

Creamos ahora todos los posibles pares candidatos: obtenemos pares
de las cubetas y calculamos pares únicos:

```{r}
# Busca los pares candidatos 
extraer_pares_candidatos <- function(lsh, textos = NULL){
   pares <- lsh %>% 
    mutate(pares = map(docs, ~ combn(sort(.x), 2, simplify = FALSE))) %>% # comb de 2 en 2 para elementos de la cubeta. 
    select(cubeta, pares) %>% unnest %>%  # convierte a dataframe
    mutate(a = map_int(pares, 1)) %>%  # primer elemento del par
    mutate(b = map_int(pares, 2)) %>%  # segundo elemento del par
    select(-pares) %>% select(-cubeta) %>% unique # se queda solo con los elementos
   if(!is.null(textos)){
       pares <- pares %>% mutate(texto_a = textos[a], texto_b = textos[b]) # extrae los textos. 
   }
   pares
}

# Encontramos los pares de artículos de wikipedia que comparten categorias. 
candidatos <- extraer_pares_candidatos(lsh_agrupados, textos) 
candidatos
```

Y nos queda por evaluar estos pares para encontrar la similitud exacta y
poder descartar falsos positivos:

```{r}
# Calcula la similitud de Jaccard de los pares candidatos. 
candidatos <- candidatos %>% mutate(sim = 
        map2_dbl(texto_a, texto_b, function(ta, tb){
            sim_jaccard(tokenize_sp(ta)[[1]], tokenize_sp(tb)[[1]])
        }))        

# Vemos cuantos pares candidatos se examinaron
nrow(candidatos)

# Filtramos los que tienen similitud de Jaccard mayor a 0.4 
cand_filtrados <- candidatos %>% filter(sim > 0.4)

# Vemos cuantos pares candidatos quedaron
nrow(cand_filtrados)

# Calculamos la distribución de la similitud de Jaccard
quantile(cand_filtrados$sim)
```

Más del 50% de la muestra tiene coincidencia exacta en las categorías. De los 36,172 candidatos, al filtrar por similitud mayor a 0.4 ya sólo tenemos 11,403 para examinar, lo que es mucho más facil de calcular. 

```{r}
# imprime el resultado
cand_filtrados
```

Nótese que este número de comparaciones es órdenes de magnitud más
chico del total de posibles comparaciones del corpus completo.

### Consultas de documentos similares {-}

Podemos usar también la estructura de cubetas para identificar candidatos
similares a un documento dado, sin tener que recorrer todos los documentos
y calcular similitud (ya sea estimada con firmas o exacta). ¿Cómo se implementaría?
Abajo veremos cómo hacerlo con el paquete *textreuse*, por ejemplo.

**Ejercicio**: (opcional) implementa esta función con el código que usamos arriba.


## Ejemplo: artículos de wikipedia (*textreuse*)

Repetimos el análsis de arriba usando *textreuse*:

### Matriz de firmas {-}

```{r}
library(textreuse)
options("mc.cores" = 4L) # para usar múltiples cores y paralelizar. 

# esta es la función que vamos a usar: separa las categorías por espacio. 
tokenize_sp <- function(x) str_split(x, ' ', simplify = TRUE)

# aunque otra opción es usar funciones hash:
minhashes <- minhash_generator(num_hashes, seed = 1223)

# Pega todas las categorías en una sola línea separada por espacio
# esta línea solo es necesaria porque TextReuseCorpus espera una
# línea de texto, no un vector de tokens.
textos <- articulos_df$categorias %>% 
          lapply(function(x) paste(x, collapse = ' ')) %>%
          as.character

# Asigna nombres de los artículos 
names(textos) <- articulos_df$articulo

# Calcula la matriz de firmas 
system.time(
wiki_corpus <-  TextReuseCorpus(
                text = textos, 
                tokenizer = tokenize_sp,
                minhash_func = minhashes,
                skip_short = FALSE)
)
```

```{r}
str(wiki_corpus[[1002]])
head(wiki_corpus[[1]])
```

### Agrupar en cubetas {-}


```{r}
# Aplica Locality Sensitive Hashing para 20 bandas de tamaño 3
# Devuelve la cubeta a la que pertenece cada artículo. 
lsh_wiki <- lsh(wiki_corpus, bands = 20)
```

```{r}
# Muestra 20 elementos de lhs_wiki
lsh_wiki %>% sample_n(20)
```


Agrupamos por cubetas y filtramos las cubetas con más de un documento:

```{r}
# Filtra cubetas con más de un documento. 
cubetas_df <- lsh_wiki %>% 
             group_by(buckets) %>% # agrupa por cubeta
             summarise(candidatos = list(doc)) %>% # obtiene lista de documentos por cubeta
             mutate(num_docs = map_int(candidatos, length)) %>% # cuenta el no. de doc en cada cubeta
             filter(num_docs > 1) # filtra cubetas con más de un documento. 
```

```{r}
# Ordena cubetas por no. de documento 
cubetas_df <- cubetas_df %>% arrange(desc(num_docs)) 

# Extrae el no. de cubetas 
nrow(cubetas_df)
```

Se tienen 12031 clusters de documentos que se parecen entre si. 

```{r}
# Muestra de cubetas 
sample_n(cubetas_df, 20)
```

Examinemos algunas de las cubetas: 

```{r}
# Examinamos los candidatos de la primer cubeta 
cubetas_df$candidatos[[1]]

# Extrae las categorías de los documentos 
lapply(cubetas_df$candidatos[[1]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```

Por ejemplo, estos documentos se parecen porque son las publicaciones de cada día del año.

```{r}
# Examina la cubeta 714 
cubetas_df$candidatos[[714]]

# Examina las cateogrías
lapply(cubetas_df$candidatos[[714]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```

```{r}
# Examina la cubeta 911
cubetas_df$candidatos[[911]]

# Extrae las categorías de los documentos
lapply(cubetas_df$candidatos[[911]], function(articulo) 
  wiki_corpus[[articulo]]$content) %>% head
```
---

## Consulta de nuevos pares candidatos.

Si tenemos un documento dado (nuevo o de la colección) 
y queremos encontrar candidatos
similares, podemos hacerlo usando la estructura de
LSH que acabamos de construir, sin tener que
recorrer todos los documentos.  Podemos hacer:

- Construimos la firma minhash del documento.
- Calculamos las cubetas donde este documento cae, y buscamos
estas cubetas que creamos para la colección
- Extraemos los elementos que están en estas cubetas.



### Ejempo: Consulta de pares candidatos {-}

Podemos buscar más fácilmente candidatos similares:

```{r}
# Buscamos artículos que se parezcan a October_1
# lhs_query recupera documentos similares de una lista de cubetass. 
lsh_query(lsh_wiki, 'October_1')
```

Hay 175 documentos que son similares a "October_1" que son básicamente las publicaciones de todo el año. 

```{r}
# Buscamos artículos similares a icosahedron
lsh_query(lsh_wiki, 'Icosahedron')
```

No parece haber mucha relación entre estos documentos. 
Veamos por qué esta última lista se ve así. Examinamos dos ejemplos, y
vemos que en efecto su similitud no es tan baja:

```{r}
# contenido de icosahhedron
wiki_corpus[["Icosahedron"]]

# contenido de disaster 
wiki_corpus[["Disaster"]]

```

```{r}
# Función para calcular en qué minhases son iguales 
minhash_estimate <- function(a, b, corpus){
  mean(corpus[[a]]$minhashes == corpus[[b]]$minhashes)
}

# Evlua la proporción de minhashes en que son iguales. 
lsh_query(lsh_wiki, 'Icosahedron') %>% # extrae art. similares a lsh_wiki
  rowwise %>% # agrega no. filas
  mutate(score = minhash_estimate(a, b, wiki_corpus)) %>% # calcula el las coliciones en minhashes
  arrange(desc(score)) # ordena por mayor no. de coliciones

```

Los más parecidos a icosahedron son aquellos que tambíen representan poliedros: dodecahhedron, octahedron, tetrahedron, etc... 


### Filtrar falsos positivos {-}

Y también podemos preprocesar todos los candidatos y eliminar
los falsos positivos:


```{r}
# Dada la lista de cubetas, devuelve los pares candidatos. 
wiki_candidatos <- lsh_candidates(lsh_wiki)
wiki_candidatos %>% nrow
```

Obtenemos 66,699 pares candidatos. Ya sólo tenemos que evaluar estos resultados (antes tendríamos
que haber evaluado alrededor de $127$ millones de pares). Calculamos
el score:

```{r}
wiki_candidatos <- 
  wiki_candidatos %>% # pares de candidatos
  rowwise %>% # agrega filas 
  mutate(score = minhash_estimate(a, b, wiki_corpus)) # calcula colicion de minhashes
wiki_candidatos %>% sample_n(20) # extrae 20 
```

```{r, fig.width=10,fig.height=4}
# vemos la distribución del score de los candidatos 
qplot(wiki_candidatos$score)
```

Tenemos muchos candidatos con similitud de 0.3 aproximadamente. Tambíen hay muchos candidatos con similitud igual a 1. 

```{r}
# filtramos para similitud mayora 0.4
candidatos_finales <- filter(wiki_candidatos, score > 0.4)

# no. final de candidatos
nrow(candidatos_finales)
```

En total, se obtienen 25,265 pares candidatos similares. 

```{r}
# Exraemos aleatoriamente unos 200 candidatos
candidatos_finales %>% 
  sample_n(200)
```

A simple vista, parecen ser coherentes los pares canddiatos con alta similitud. Examinamos algunos en específico.

```{r, message = FALSE}
lsh_query(lsh_wiki, 'Economy_of_Paraguay') %>% # extrae todos los similares a "Economy of Paraguay"
  left_join(candidatos_finales) %>% # Pega el score
  filter(!is.na(score)) # elimina lso que no tienen score porque no fueron candidatos finales. 
```


```{r, message = FALSE}
lsh_query(lsh_wiki, 'Icosahedron') %>% # extrae todos los similares a "Icosahedron"
  left_join(candidatos_finales) %>% # pega el score
  filter(!is.na(score)) # elimina los que no tienen score porque no fueron candidatos finales. 
```


```{r, message = FALSE}
lsh_query(lsh_wiki, 'Ghana') %>% # extrae todos los similares a "Ghana"
  left_join(candidatos_finales) %>% # pega el score
  filter(!is.na(score)) # elimina los que no tienen score porque no fueron candidatos finales. 
```

En este último ejemplo se tienen puros países con catogrías similres a las de Ghana (como países africanos, paises limitantes con el atlántico, paises de habla inglesa, economías menos desarrolladas, etc) 

## Candidatos idénticos

Cuando buscamos candidatos idénticos, podemos intentar
otras estrategias. Si los documentos no son muy grandes, podemos
hacer hash del documento entero a un número grande de cubetas. Si el
número de cubetas es suficientemente grande para la colección
de texto, entonces cualquier par de documentos que caigan en una 
misma cubeta serán idénticos con muy alta probabilidad.

Para documentos más grandes, podemos tomar, por ejemplo, una selección
al azar de posiciones en el documento y usar funciones hash. Puedes
revisar otras técnicas en [@mmd], Sección 3.9

### Ejemplo {-}

En el caso de artículos de wikipedia vimos algunas cubetas de
artículos que contenían exactamente las mismas categorías. Podríamos
hacer, por ejemplo:

```{r}
# Datos orignales 
articulos_df

# Aplica funciones hash para las categorías en cada artículo 
hash_categorias <- articulos_df %>% 
                   mutate(hash_doc = map_chr(categorias, 
                                    function(x) digest::digest(sort(x)))) 

# Busca los artículos de abril 1 y abril 10 
hash_categorias %>% filter(articulo %in% c('April_1','April_10')) 
```

Vemos que los documentos 'April\_1' y 'April\_10' tienen el mismo hash, es decir, pertenecen a las mismas categorías:

```{r}
aux<-hash_categorias %>% filter(articulo %in% c('April_1','April_10'))
aux$categorias
```


```{r}
# Agrupa los documentos por su hash completo
hash_categorias <- hash_categorias %>%
                   select(hash_doc, articulo) %>% # quita la col. de categorias
                   group_by(hash_doc) %>% # agrupa por hash
                   summarise(articulo = list(articulo)) %>% # lista los artículos en cada hash
                   mutate(num_docs = map_int(articulo, length)) %>% # cuenta el no. de artículos en cada hash
                   filter(num_docs > 1) %>% # filtra hashes con mas de un doc. 
                   arrange(desc(num_docs)) # ordena 

# Vemos los hashes que tienen mayor no. de docuemtnos 
hash_categorias
```

De esta tabla podemos obtener los pares idénticos:

```{r}
# Extrae los artículos del segundo hash más frecuente
hash_categorias$articulo[[2]]

# Extrae las categorías a las que pertenecen
articulos_df %>% filter(articulo == 'Abingdon') %>% pull(categorias)
```

Son artículos que se utilizan para eliminar ambiguedad en las búsquedas. Por ejemplo Buffalo de ciudad o Buffalo de animal. 


Y podemos eliminar los pares idénticos de nuestros candidatos previos, para reducir el trabajo 
de cálculo:

```{r}
# lista de artículos con pares idénticos
df_1 <- data_frame(a = unlist(hash_categorias$articulo)) 

# obtiene los candidatos que no son pares idénticos
wiki_candidatos_noid <- wiki_candidatos %>%
                        anti_join(df_1) %>% # extrae los candidatos que no aparecen en los identicos
                        anti_join(df_1 %>% rename(b=a)) # extrae los candidatos que no aparecen en los identicos y renombra columnas.

# distribución de la similitud de los no identicos
qplot(wiki_candidatos_noid$score)
```

Ya no aparecen documentos con similitud igual a 1. 

## Medidas de distancia

La técnica de LSH puede aplicarse a otras medidas de distancia, con
otras formas de hacer hash diferente del minhash. La definición
de distancia puedes consultarla [aquí](https://en.wikipedia.org/wiki/Metric_(mathematics))

### Distancia de Jaccard

Puede definirse simplemente como 
$$1-sim(a,b),$$
donde $a$ y $b$ son conjuntos y $sim$ es la similitud de Jaccard.

### Distancia euclideana

Es la distancia más común para vectores de números reales:

Si $x=(x_1,\ldots, x_p)$ y $y=(y_1,\ldots, y_p)$ son dos vectores,
su norma $L_2$ está dada por

$$ d(x,y) = \sqrt{\sum_{i=1}^p (x_i-y_i)^2  } = ||x-y||$$

### Distancia coseno

La distancia coseno, definida también para vectores de números reales, no toma en cuenta la magnitud de vectores, sino solamente su dirección.

La similitud coseno se define primero como
$$sim_{cos}(x,y) = \frac{<x, y>}{||x||||y||} = \cos (\theta)$$
donde $<x, y> = \sum_{i=1}^p x_iy_i$ es el producto punto de $x$ y $y$. Esta cantidad es igual al coseno del ángulo entre los vectores $x$ y $y$ .

Demostración: 
$$
cos(\theta)=\dfrac{c. Adyacente}{Hipotenusa}=\dfrac{||\text{proyeccion de y sobre x||}}{||y||}=\dfrac{\dfrac{<x,y>}{||x||}}{||y||}=\dfrac{<x,y>}{||x||||y||}
$$


La distancia coseno es entones
$$
d_{cos}(x,y) = 1- sim_{cos}(x,y).
$$

Esta distancia es útil cuando el tamaño general de los vectores no nos importa. Como veremos más adelante, una aplicación usual es comparar
documentos según las frecuencias de los términos que contienen: en este
caso, nos importa más la frecuencia relativa de los términos que su frecuencia absoluta (pues esta última también refleja la el tamaño de los documentos).

A veces se utiliza la distancia angular (medida con un número entre 0 y 180), que se obtiene de la distancia coseno, es decir,
$$d_a(x,y) = \theta,$$
donde $\theta$ es tal que $\cos(\theta) = d_{cos}(x,y).$

### Distancia de edición

Esta es una medida útil para medir distancia entre cadena. La
distancia de edición entre dos cadenas $x=x_1\cdots x_n$ y 
$y=y_1\cdots y_n$ es el número mínimo de inserciones y eliminaciones (un caracter a la vez) para convertir a $x$ en $y$. 

Por ejemplo, la distancia entre "abcde" y "cefgh" se calcula como
sigue: para pasar de la primera cadena, necesitamos agregar $f$, $g$ y $h$ ($3$ adiciones), eliminar $d$, y eliminar $a,b$ ($3$ eliminaciones). La distancia entre estas dos cadenas es $6$.


## Teoría de funciones sensibles a la localidad


Vimos como la familia de funciones minhash puede combinarse (usando 
la técnica de bandas) para discriminar entre pares de baja similitud
y de alta similitud.

En esta parte consideramos otras posibles familias de funciones para lograr 
lo mismo (hacer LSH), bajo otras medidas de distancia. Veamos las características básicas de las funciones minhash:

 1. Cuando la distancia entre dos elementos $x,y$ es baja (similitud alta),
 entonces $f(x)=f(y)$ tiene probabilidad alta. 
 2. Podemos escoger varias funciones  $f_1,\ldots,f_k$ con la propiedad anterior, de manera independiente, de forma que es posible calcular (o acotar)
 la probabilidad de $f_i(x)=f_i(y)$ .
 3. Las funciones tienen que ser relativamente fáciles de calcular (comparado con calcular todos los posibles pares y sus distancias directamente).
 
 
## Funciones sensibles a la localidad


```{block2, type="resumen"}
Sean $d_1<d_2$ dos valores (que interpretamos como distancias).

Una familia ${\cal F}$ es una familia $d_1,d_2,p_1,p_2$,  sensible a localidad
(con $p_1>p_2$) cuando para cualquier par de elementos $x,y$,

1. Si $d(x,y)\leq d_1$, entonces la probabilidad  $P(f(x)=f(y))\geq p_1$. 
2. Si $d(x,y)\geq d_2$, entonces $P(f(x)=f(y))\leq p_2$ 

Nótese que las probabilidades están dadas sobre la selección de $f$.
```
  


Estas condiciones se interpretan como sigue: cuando $x$ y $y$ están
suficientemente cerca ($d_1$), la probabilidad de que sean mapeados al mismo valor
por una función $f$ de la familia es alta.  Cuando $x$ y $y$ están lejos
$d_2$, entonces, la probabilidad de que sean mapeados al mismo valor es baja.

Es decir, distancias pequeñas tienen mayor probabilidad de colición mientras que distancias grandes tienen menor probabilidad de colición. 


Podemos ver una gráfica:   


```{r, echo = FALSE}
x_1 <- seq(0, 1, 0.01)
x_2 <- seq(2, 3, 0.01)
y_1 <- -1*x_1 + 2.5
y_2 <- 2/x_2

dat_g <- data_frame(x=c(x_1,x_2),y=c(y_1,y_2))
ggplot(dat_g, aes(x=x, y=y)) + geom_point(size=0.5) +
  geom_vline(xintercept=c(1,2), linetype="dotted") +
  geom_hline(yintercept=c(1,1.5), linetype="dotted") +
  scale_x_continuous(breaks = c(1,2), labels = c('d_1','d_2')) +
  scale_y_continuous(breaks = c(1,1.5), labels = c('p_2','p_1')) +
  labs(x = 'Distancia', y ='Probabilidad de candidato')
```

### Distancia Jaccard

Supongamos que tenemos dos documentos $x,y$. Si ponemos por ejemplo
$d_1=0.2$ y $d_2= 0.5$, tenemos que 
si $d(x,y) = 1-sim(x,y) \leq 0.2$, despejando tenemos
$sim(x,y)\geq 0.8$, y entonces
$$P(f(x) = f(y)) = sim(x,y) \geq 0.8$$
Igualmente, si $d(x,y)=1-sim(x,y) \geq 0.5$, entonces
$$P(f(x) = f(y)) = sim(x,y) \leq 0.5$$
de modo que la familia de minhashes $(0.2,0.5,0.8,0.5)$ es sensible a la
localidad.

```{block2, type="resumen"}
Para cualquier $d_1 < d_2$,
la familia de funciones minhash  
$(d_1, d_2, 1-d_1, 1-d_2)$ es una familia sensible a la localidad para cualquier
$d_1\leq d_2$.
```

## Amplificación de familias sensibles a la localidad

Con una familia sensible a la localidad es posible usar la técnica
de bandas para obtener la discriminación de similitud que nos interese.

Supongamos que ${\cal F}$ es una familia $(d_1, d_2, p_1, p_2)$-sensible
a la localidad. Podemos usar **conjunción** de ${\cal F}'$ para construir
otra familia sensible a la localidad.

Sea $r$ un número entero. Una función $f\in {\cal F}'$ se construye
tomando $f = (f_1,f_2,\ldots, f_r)$, con $f_i$ seleccionadas al
azar de manera independiente de la familia original, de forma
que $f(x)=f(y)$ si y sólo si $f_i(x)=f_i(y)$ para toda $i$. Esta construcción
corresponde a lo que sucede dentro de una banda de la técnica de LSH.

La nueva familia ${\cal F}'$ es $(d_1,d_2,p_1^r,p_2^r)$ sensible a la localidad. Nótese que las probabilidades siempre se hacen más chicas
cuando incrementamos $r$, lo que hace más fácil discriminar pares
con similitudes en niveles bajos.


Podemos también hacer **disyunción** de una familia  ${\cal F}$. En este
caso, decimos que $f(x)=f(y)$ cuando al menos algún
$f_i(x)=f_i(y)$.

En este caso, la disyunción da una familia
$(d_1,d_2,1-(1-p_1^r)^b,1-(1-p_2^r)^b)$ sensible a la localidad. Esta construcción
es equivalente a construir varias bandas.

La idea general es ahora:

```{block2, type="resumen"}
- Usando **conjunción**, podemos construir una familia donde
la probabilidad $p_2^r$ sea mucho más cercana a cero que
$p_1^r$ (en términos relativos). 

- Usando **disyunción**, podemos construir una familia donde
la probabilidad $1-(1-p_1^r)^b$ permanece cercana a $1$,
pero $1-(1-p_2^r)^b$ está cerca de cero.

- Combinando estas operaciones usando la técnica de bandas
podemos construir una famlia que discrimine de manera distinta
entre distancias menores a $d_1$ y distancias mayores a $d_2$.

- El costo incurrido es que tenemos que calcular más funciones para
discriminar mejor.
```

### Ejercicio {-}
Supongamos que tenemos una familia $(0.2, 0.6, 0.8, 0.4)$ sensible
a la localidad. Si combinamos, con conjunción, 4 de estas funciones
obtenemos una familia
$$(0.2, 0.6, 0.8^4, 0.4^4)=(0.2, 0.6, 0.41, 0.026)$$
La proporción de falsos positivos es chica, pero la de falsos negativos
es grande. Si tomamos 8 de estas funciones (cada una compuesta de
cuatro funciones de la familia original) y hacemos conjunción, obtenemos una familia

$$[0.2, 0.6, 1-(1-0.8^4)^8, 1-(1-0.4^4)^8]=[0.2, 0.6, 0.98, 0.19]$$

En esta nueva familia, tenemos que hacer $40$ veces más trabajo para
tener esta amplificación.

---

## Distancia coseno e hiperplanos aleatorios

Construimos ahora LSH para datos numéricos, y comenzaremos
con la distancia coseno. Lo primero que necesitamos es
una familia sensible a la localidad para la distancia coseno.

Consideremos dos vectores, y supongamos que el ángulo entre ellos
es chico. Si escogemos un hiperplano al azar, lo más
probable es que queden del mismo lado del hiperplano. En el caso extremo, 
si los vectores
apuntan exactamente en la misma dirección, entonces la probabilidad es $1$.

Sin embargo, si el ángulo entre estos vectores es grande, entonces lo más probable es que queden separados por un hiperplano escogido al azar. Si los vectores son ortogonales (máxima distancia coseno posible), entonces
la probabilidad de quedar del mismo lado del hiperplano es $0$.

Esto sugiere construir una familia sensible a la localidad para
la distancia coseno de la siguiente forma:

- Tomamos un vector al azar $v$.
- Nos fijamos en la componente de la proyección de $x$ sobre $v$ 
- Ponemos $f_v(x)=1$ si esta componente es positiva, y
$f_v(x)=-1$ si esta componente es negativa.
- Podemos poner simplemente:
$$ f_v(x) = signo (<x, v>)$$

**Recordatorio**: La componente de la proyección de $x$ sobre $v$ está
dada por el producto interior de $x$ y $v$ normalizado:
$$\frac{1}{||v||}<x, v>,$$
y su signo es el mismo de $<x,v>$.


```{block2, type="resumen"}
La familia descrita arriba (hiperplanos aleatorios) 
es $(d_1,d_2, (180-d_1)/180, d_2/180)$
  sensible a la localidad para la distancia angular.
```

Vamos a dar un argumento del cálculo: supongamos que el ángulo entre $x$ y $y$ es $d=\theta$, es decir,
la distancia angular entre $x$ y $y$ es $\theta$.  

Consideramos el plano $P$ que pasa por el origen y por $x$ y $y$. 
Si escogemos un vector al azar (cualquier dirección igualmente probable), el vector produce un hiperplano perpendicular (son
los puntos $z$ tales que $<z,v>=0$)
que corta al plano $P$
en dos partes.
Todas las direcciones de corte son igualmente probables, así
que la probabilidad de que la dirección de corte separe a $x$ y $y$
es igual a $2\theta /360$ (que caiga en el cono generado por $x$ y $y$).
Si la dirección de corte separa a $x$ y $y$, entonces sus valores
$f_v(x)$ y $f_v(y)$ no coinciden, y coinciden si la dirección
no separa a $x$ y $y$. Así que:

1. $d(x,y)=d_1=\theta$, entonces $P(f(x)=f(y)) = 1-(2d_1/360)=1-(d_1/180).$

Por otro lado, 

2. $d(x,y)=d_2$, entonces  $P(f(x)\neq f(y)) = 2d_2/360=d_2/180.$

---

### Ejemplo: similitud coseno por fuerza bruta {-}

Comenzamos con un ejemplo simulado.

```{r}
# Fija la semilla 
set.seed(101)
# Simula 2 matrices 
mat_1 <- matrix(rnorm(300 * 1000) + 3, ncol = 1000)
mat_2 <- matrix(rnorm(600 * 1000) + 0.2, ncol = 1000)

# Crea un data frame, agrega un id. 
df <- rbind(mat_1, mat_2) %>% data.frame %>%
           add_column(id_1 = 1:900, .before = 1)

# Tenemos un data frame de 900 observaciones y 1001 columnas
dim(df)

# Veamos un pedacito. 
head(df[,1:5])
```

Tenemos entonces $1000$ variables distintas y $900$ casos, y nos
interesa filtrar aquellos pares de similitud alta.

Definimos nuestra función de distancia

```{r}
# Calcula la norma de un vector 
norma <- function(x){
  sqrt(sum(x ^ 2))
}

# Calcula la distancia coseno
dist_coseno <- function(x, y){
  1 - sum(x*y) / (norma(x) * norma(y))
}
```


Y calculamos todas las posibles distancias (normalmente
 **no** queremos hacer esto, pero lo hacemos aquí para
 comparar):

```{r}
# Genera un data frame con las 1000 variables para cada obs en forma de lista
df_agrup <- df %>% gather('variable', 'valor', -id_1) %>%
                   group_by(id_1) %>%
                   arrange(variable) %>%
                   summarise(vec_1 = list(valor))

# Crea las posibles parejas de observaciones  
df_pares <- df_agrup %>% 
            crossing(df_agrup %>% 
                       rename(id_2 = id_1, vec_2 = vec_1)) %>% # lista posibles parejas en un data frame 
            filter(id_1 < id_2) %>% # elimina repetidos
            mutate(dist = map2_dbl(vec_1, vec_2, dist_coseno)) # calcula dist. coseno 

# Primeras 1000 observaciones 
df_pares
```

La distribución de distancias sobre todos los pares es la siguiente:

```{r, fig.width=5, fig.asp = 0.8}
qplot(df_pares$dist, binwidth = 0.01)
```

¿Por qué observamos este patrón? Recuerda que esta gráfica
 representa pares. Esto lo observamos por la forma en que se simularon los datos. Los pares que vienen de m1 vs m1 tienen poca distancia, al igual que los vienen de m2 vs m2. Sin embargo, los que vienen de m1 vs m2 o m2 vs m1 tienen una mayor distancia porque m1 se simuló alrededor de 3 y m2 alrededor de 0.2. 
 
 
Y supongamos que queremos encontrar vectores con distancia
coseno menor a $0.2$ (menos de unos $40$ grados). El número de pares que satisfacen
esta condicion son:


```{r}
# Filtramos parejas de observacions con distancia coseno menor a 0.2
sum(df_pares$dist < 0.20)
```


### Ejemplo: LSH planos aleatorios {-}

Si en lugar de usar la fuerza bruta utilizamos $200$ funciones hash:

```{r}
# Fija la semilla 
set.seed(101021)

# Simula 200 funciones hash simulando un vector normal y calculando
# el producto punto y se queda con el signo del mismo. 
hashes <- lapply(1:200, function(i){
    v <- rnorm(1000)
    function(x){
        ifelse(sum(v*x) >= 0, 1, -1) 
    }
})
```

Por ejemplo, la firma del primer elemento es:

```{r}
# Extrae los datos para la primer observacion
x <- as.numeric(df[1,-1])

# Aplica los hashes
sapply(hashes, function(f) f(x))
```


Y ahora calcuamos la firma para cada elemento:

```{r}

df_hash <- df_agrup %>%
           mutate(df = map(vec_1, function(x){
              firma <-  sapply(hashes, function(f) f(x)) # calcula el vector de firmas 
              data_frame(id_hash = 1:length(firma),
                         firma = firma) })) %>%  # junta info en data frame 
              select(-vec_1) %>% unnest # elimina el vector simulado y crea dataframe 

# matriz de firmas por elemento.
df_hash
```


Vamos a amplificar la famiia de hashes. En este caso,
escogemos $20$ bandas de $10$ hashes cada una.

```{r, fig.width=5, fig.asp=0.8}
# función para amplificar hashes. 
f_1 <- function(x){
    1-(1-((180-x)/180)^10)^20
}
# grafica la curva. 
curve(f_1, 0, 180)

# línea en 20
abline(v=20)
```


### Ejemplo: agrupar por cubetas para LSH {-}

Ahora agrupamos y construimos las cubetas:

```{r}
# Extrae la banda correspondiente, pega el id.hash y la firma 
df_hash_1 <- df_hash %>% 
           mutate(banda  = (id_hash - 1) %% 20 + 1) %>%
           mutate(h = paste(id_hash,firma)) %>%
           arrange(id_1)
df_hash_1
```


```{r}
# Cera las cubetas agrupando por id_1 y banda y pegando la h.  
cubetas <- df_hash_1 %>% 
             group_by(id_1, banda) %>%
             summarise(cubeta = paste(h, collapse = '/')) 
cubetas
```

```{r}
# Aplica una función hash para las cubetas 
# para que sea un no. más manejable
cubetas_hash <- cubetas %>%
                ungroup %>% rowwise %>%
                mutate(cubeta = digest::digest(cubeta))
cubetas_hash
```


```{r}
# Agrupa por cubetas y cuenta el no de observaciones 
cubetas_agrup <- cubetas_hash %>% group_by(cubeta) %>%
                  summarise(ids = list(id_1)) %>%
                  mutate(num_ids = map_dbl(ids, length)) %>%
                  filter(num_ids > 1 )

cubetas_agrup
```

Y ahora extraemos los pares similares

```{r}
# Encuentra pares candidatos 
pares_candidatos <- lapply(cubetas_agrup$ids, function(x){
  combn(sort(x), 2, simplify = FALSE)}) %>% # combinaciones de obs de 2 en 2
  flatten %>% unique %>% # quita repetidos 
  transpose %>% lapply(as.integer) %>% as.data.frame # transpone, convierte a entero y a data frame 

# asigna nombres de columnas 
names(pares_candidatos) <- c('id_1','id_2')

# primeros pares candidatos 
head(pares_candidatos)
```


### Ejemplo: filtrar y evaluar resultados {-}

Y ahora evaluamos nuestros resultados. En primer lugar, el 
número de pares reales y de candidatos es

```{r}

# Filtramos pares con distnacia menor a 0.20
pares_reales <- filter(df_pares, dist < 0.20) %>%
                select(id_1, id_2)

# Pares candidatos reales con distancia baja. 
nrow(pares_reales)

# Total de pares candidatos. 
nrow(pares_candidatos)
```

Así que debemos tener buen número de falsos positivos. Podemos calcularlos haciendo
```{r}
# Extrae el no. de falsos positivos. 
nrow(anti_join(pares_candidatos, pares_reales))
```

Y el número de falsos negativos es

```{r}
# Extrae el no. de falsos negativos 
nrow(anti_join(pares_reales, pares_candidatos))
```

que es un porcentaje bajo del total de pares reales.


---


**Observación*: es posible, en lugar de usar vectores con dirección
aleatoria $v$ escogidos al azar como arriba (con la distribución normal), hacer menos cálculos escogiendo vectores $v$
cuyas entradas son solamente $1$ y $-1$. El cálculo del producto
punto es simplemente multiplicar por menos si es necesario los
valores de los vectores $x$ y sumar.

## LSH para distancia euclideana.

Para distancia euclideana usamos el enfoque de proyecciones
aleatorias en cubetas.

La idea general es que tomamos una línea al azar en el espacio
de entradas, y la dividimos en cubetas de manera uniforme. El valor
hash de un punto $x$ es el número de cubeta donde cae la proyección de $x$.

```{block2, type='resumen'}
Supogamos que tomamos como $a$ el ancho de las cubetas.
La familia de proyecciones aleatorias por cubetas es
una familia
$(a/2, 2a, 1/2, 1/3)$-sensible a la localidad para la distancia 
euclideana.
```

Supongamos que dos puntos $x$ y $y$ tienen distancia euclideana
$d = a/2$. Si proyectamos perpendicularmente sobre la línea
escogida al azar, la distancia entre las proyecciones es menor
a $a/2$, de modo la probabilidad de que caigan en la misma
cubeta es al menos $1/2$. Si la distancia es menor, entonces la probabilidad
es más grande aún:

1. Si $d(x,y)\leq a/2$ entonces $P(f(x)=f(y))\geq 1/2$.

Por otro lado, si la distancia es mayor a $2a$, entonces la única
manera de que los dos puntos caigan en una misma cubeta es
que la distancia de sus proyecciones sea menor a $a$. Esto sólo
puede pasar si el ángulo entre el vector que va de $x$ a $y$ y
la línea escogida al azar es mayor de  $60 \grad$ a $90\grad$. Como
$\frac{90\grad-60\grad}{90\grad-0\grad} = 1/3$, entonces la probabilidad que que
caigan en la misma cubeta no puede ser más de $1/3$.

1. Si $d(x,y)\geq 2a$, entonces $P(f(x)=f(y))\leq 1/3$.

Escoger $a$ para discriminar las distancias que nos interesa,
y luego amplificar la familia para obtener tasas de falsos
positivos y negativos que sean aceptables.


## Tarea {-}

Ver 

1. Minhashing con spark *scripts/lsh/minhash_spark.Rmd*.
2. Un ejercicio para encontrar artículos duplicados *scripts/lsh/entity-matching.Rmd*



