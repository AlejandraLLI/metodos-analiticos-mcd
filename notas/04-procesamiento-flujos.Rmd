# Procesamiento de flujos

En esta parte supondremos que los datos se pueden representar como un flujo de tal velocidad y volumen que típicamente no es posible almacenar en memoria todo el flujo, o más en general, que para nuestros propósitos sería lento hacer 
queries a la base de datos resultante. Veremos técnicas simples para obtener resúmenes simples y rápidos de flujos grandes, y también veremos cómo aplicar métodos probabilísticos para filtrar o resumir ciertos aspectos de estos flujos.

Ejemplos de flujos que nos interesan son: logs generados por visitas y transacciones en sitios de internet, datos de redes de sensores, 
o transacciones en algún sistema.

Para analizar flujos con estas propiedades podemos hacer:

- Restricción temporal: considerar ventanas de tiempo, y hacer análisis sobre los últimos datos en la ventana. Datos nuevos van reemplazando a datos anteriores, y puede ser que los datos anteriores no se respaldan (o es costoso acceder a ellos).

- Resúmenes acumulados: guardamos resúmenes de los datos que podemos actualizar y utilizar para calcular características de interés en el sistema, por ejemplo: conteos simples, promedios. Algunos resúmenes son más difíciles de hacer eficientemente: por ejemplo, número de elementos únicos del flujo.

- Muestreo probabilístico: Podemos diseñar muestras apropiadas para estimar cantidades que nos interesen, y sólo guardar los datos que corresponden a la muestra.

- Filtrado: cómo retener para análisis elementos del flujo que satisfagan alguna propiedad de interés.



## Selección de muestras y funciones hash

Dependiendo de qué nos interesa medir en un flujo podemos decidir cuáles
son las unidades que es necesario muestrear. Típicamente la unidad de un
flujo no corresponde a las unidades que nos interesan. Por ejemplo: en *logs*
de sitios web, las unidades observamos en el flujo son transacciones muy granulares (clicks, movimientos de mouse, envío de datos, etc.), pero nos interesa obtener
propiedades a nivel de usuario, o sesión, etc.

Dependiendo de las unidades de muestreo apropiadas que nos
interesen (por ejemplo, clientes o usuarios, transacciones, etc.)
podemos diseñar distintas estrategias.

### Ejemplo {-}
Si nos interesa estimar el promedio del tamaño de las transacciones en una ventana de tiempo dada, podemos muestrar esa ventana. Cada vez que llega una transacción, usamos un número aleatorio para decidir si
lo incluimos en la muestra o no, y luego hacer nuestro análisis
con las unidades seleccionadas.


```{r, message = FALSE, warning = FALSE}
# Cargar librerías
library(tidyverse)
```

```{r, echo=FALSE, message=FALSE}
theme_set(theme_bw())
cb_palette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
```


```{r}
# Función para generar transacciones
generar_trans <- function(...){
  
  # Muestreo aleatorio simple de los id's
  id_num <- sample.int(10000, 1)
  
  # Simula el monto con una t-student
  monto <- rt(1, df = 5, ncp = 5000) 
  
  # Genera un dataframe con el id y el monto de la transacción. 
  trans <- list(id = id_num, monto = monto)
  
  # Regresa el dataframe trans
  trans
}
```

Ahora simulamos un flujo y calculamos la mediana, con todos los datos:

```{r}
# Fija la semilla. 
set.seed(312)

# Genera 100 mil transacciones 
trans <- map(1:100000, generar_trans) 

# Convierte la variable monto en double
total_montos <- map_dbl(trans, "monto")

# Calcula la mediana 
median(total_montos)
```


Si queremos seleccionar un 1\% de las transacciones para hacer más rápido nuestro cálculo, podemos seleccionar al azar, para cada elemento, si lo
incluímos en la muestra o no, por ejemplo:


```{r}
# Función para decidir si se incluye el elemento o no. 
seleccionar_rng <- function(prop = 0.01){
  
  # Se genera un aleatorio entre 0 y 1, si es menor a prop
  # se incluye, de lo contrario, no se incluye. 
   runif(1) < prop
}

# Selecciona una muestra de 1% de los datos 
trans_filtradas <- keep(trans, ~ seleccionar_rng(prop = 0.01))

# No  de observaciones filtradas
length(trans_filtradas)

# Convierte a double el monto y extrae la mediana. 
trans_filtradas %>% map_dbl("monto") %>% median
```

Utilizando todos los datos, la mediana verdadera es de 5,365.75; en cambio, utilizando el 1% de la muestra la mediana es de 5,377.825. Es decir, obtenemos una muy buena aproximación de la medida de interés sin tener que guardar todos los datos. 


Este esquema simple no funciona bien cuando nuestra unidad de análisis
no corresponde a las unidades del flujo, como en este ejemplo. ¿Puedes dar ejemplos? Por ejemplo, cuando los flujos corresponden a los clicks en páginas de internet, pero lo que tu quieres medir es sobre el número de nuevos visitantes. Si se realiza un muestreo aleatorio, puedes estar dejando escapar nuevas visitas y sólo seleccionar clicks de un mismo cliente. 



---



### Ejemplo {-}

Ahora supongamos que queremos estimar el promedio de la 
transacción máxima *por cliente*  en una ventana de tiempo dada. 
En este caso, la unidad de muestreo más simple es el cliente, y el 
método del ejemplo anterior es menos apropiado. Quisiéramos en lugar de eso
tomar una muestra de clientes en la ventana, tomar el máximo de todas sus 
transacciones, y luego promediar. 

- En este caso, el análisis es más complicado si seleccionamos cada transacción 
según un número aleatorio (pues en la muestra resultante
distintos clientes tendrán distintas probabilidades de inclusión, dependiendo
de cuántas transacciones hagan en la ventana de tiempo).

```{block2, type ='resumen'}
Podemos usar una función hash del **identificador único de cliente**, y mapear
con una función hash
a un cierto número de cubetas $1,\ldots, B$. Los clientes de la muestra son los 
que caen en las cubetas $1,2,\ldots, k$, y así
obtendríamos una muestra que consiste de $k/B$ de los clientes
totales que tuvieron actividad en la ventana de interés. Almacenamos todas
las transacciones en la ventana de interés para los clientes seleccionados.
```

Con esta estrategia:

- Todos los clientes que tuvieron actividad en la ventana tienen la misma 
probabilidad de ser seleccionados.
- No es necesario buscar en una lista si el cliente está en la muestra seleccionada o no (lo cual puede ser lento, o puede ser que terminemos con muestras muy grandes o chicas).
- Podemos escoger $k$ para afinar el tamaño de muestra que buscamos.
- Este método incorpora progresivamente nuevos clientes a la lista muestreada. Por ejemplo, si la cantidad de clientes está creciendo,
entonces el número de clientes muestreados crecerá de manera correspondiente. Podemos
empezar escogiendo $A$ de $B$ cubetas (con $B$ grande), y si la muestra de cientes
excede el tamaño planeado, reducir a $A-1$ cubetas, y así sucesivamente.

Primero veamos el resultado cuando utilizamos todos los clientes de
la ventana de tiempo:

```{r}
# Imprime una línea 
sprintf("Número de clientes: %i", length(unique(map_int(trans, "id"))))

# Agrega id de filas a las transacciones
trans_df <- trans %>% bind_rows() 

# Calcula la mediana de los montos máximos de transaccion por id
mediana_max <- trans_df %>% # datos
  group_by(id) %>% # agrupa por id 
  summarise(monto_max = max(monto)) %>% # extrae monto max
  pull(monto_max) %>% # extrae un monto máximo
  median # calcula la mediana de los montos maximos

sprintf("Mediana de máximo monto: %.1f", mediana_max)
```

¿Cómo funciona si quisiéramos usar una muestra? Usamos una función hash y repartimos en 10 cubetas (deberíamos
obtener alrededor del 10\% de los clientes). Seleccionamos una sola cubeta
y la usamos para resumir:

```{r}
# Función hash para seleccionar clientes
seleccionar <- function(id){
  
  # función hash modular con módulo 10 al final para tener 10 cubetas.
  ((28*id + 110) %% 117) %% 10  == 0
}

# Selecciona clientes que caen en la cubeta 10. 
trans_filtradas <- keep(trans, ~ seleccionar(.x$id))

# Imprime el no. de clientes en la muestra
sprintf("Número de clientes: %i", length(unique(map_int(trans_filtradas, "id"))))

# Agrega no. de renglón
trans_df <- trans_filtradas %>% bind_rows() 

# Calcula la mediana de los montos máximos a partir de la muestra seleccionada. 
mediana_max <- trans_df %>% # datos  
  group_by(id) %>% # agrupa por cliente
  summarise(monto_max = max(monto)) %>% #calcula monto max.
  pull(monto_max) %>% # extrae la variable monto_max
  median # calcula la mediana

# Imprime la mediana del monto maximo para la muestra 
sprintf("Mediana de máximo monto: %.1f", mediana_max)
```

Utilizando toda la muestra la mediana del monto máximo de transacciones de un cliente es de 9,652.1; mientras que con el 10% de la muestra (aprox.) la mediana estimada es de 9,781.90.


Sin embargo, esto no funciona si seleccionamos al azar las transacciones. En este caso, obtenemos
una mala estimación con sesgo alto:

```{r}
# Filtra observaciones con MAS. 
trans_filtradas <- keep(trans, ~ seleccionar_rng(prop = 0.10))

# No. de transacciones seleccionadas
length(trans_filtradas)

# Agrega no de filas 
trans_df <- trans_filtradas %>% bind_rows() 

# Calcula la mediana del monto maximo para la muestra aleatoria
mediana_max_incorrecta <- trans_df %>% # datos
  group_by(id) %>% # agrupa por cliente
  summarise(monto_max = max(monto)) %>% # calcula monto max. 
  pull(monto_max) %>% # extrae variable 
  median # obtiene monto maximo. 

# Imprime el resultado 
sprintf("Mediana de máximo monto: %.1f", mediana_max_incorrecta)
```

Si se utiliza una muestra aleatoria de los datos, la mediana estimada del monto máximo de transacción es de 5,975.50 que es mucho menor al verdadero valor. 

**Observación**: 

1. En este último ejemplo, para cada usuario sólo
muestreamos una fracción de sus transacciones. En algunos casos, 
no muestreamos el máximo, y esto produce que la estimación 
esté sesgada hacia abajo.
2. Para un enfoque más general (por ejemplo id's que son cadenas), podemos
usar alguna función hash de digest (ver ejemplos más abajo).

## Selección de muestra bajo identificadores fijos.

En algunos casos, podemos tener un conjunto $S$ de unidades
que seleccionamos con anterioridad (no necesariamente de forma aleatoria), 
y quisiéramos seleccionar
en la muestra los datos relacionados con elementos en ese conjunto.

Por ejemplo, quizá nos interesaría muestrar las transacciones
que se hacen en comercios donde han existido fraudes anterioremente,
o quizá checar si un correo proviene de una dirección que está 
en una whitelist (o blacklist).

En estos ejemplos, 
tenemos que checar contra la lista $S$ si seleccionamos un elemento o no, a
diferencia de los ejemplos de arriba.
Cuando la lista es muy grande, esta operación puede ser costosa. 
Aquí veremos
un método probabilístico (Bloom filters) que tiene ventajas en cuanto
a memoria usada y velocidad de búsqueda, con la penalización de posibles
falsos positivos.

### Ejemplos {-}


- En este ejemplo de [Medium](https://blog.medium.com/what-are-bloom-filters-1ec2a50c68ff) 
se usan filtros de Bloom para evitar volver a recomendar artículos ya
vistos o recomendados.

- [Este es un ejemplo](https://gallery.shinyapps.io/087-crandash/) de una
aplicación que cuenta el número de usuarios únicos que bajan paquetes de CRAN. Cada
vez que hay una nueva descarga (transacción) debemos decidir si se trata
de usuarios nuevos o no y actualizar correctamente el conteo de usuarios únicos.

- Supongamos que tenemos un diccionario de palabras $S$ del español.
Cuando observamos una nueva "palabra" que alguien escribió,
queremos saber si la palabra está en el diccionario. Por ejemplo,
para decidir si es un posible error de ortografía o proponer algún sustituto.

- Decidir si una dirección web está en una lista negra, para dar una advertencia
inmediata (*safe browsing*).

- Evitar procesos duplicados en pipelines de datos, como en 
[este ejemplo](https://cloud.google.com/blog/products/gcp/after-lambda-exactly-once-processing-in-cloud-dataflow-part-2-ensuring-low-latency)


---

Una solución a este problema es el *filtro de Bloom*, que es
un esquema probabilístico para filtrar elementos de un flujo
que pertenecen a una colección fija $S$.

## Filtro de Bloom

Consideremos entonces el problema de filtrar de un flujo solamente los elementos 
que pertenezcan a un conjunto $S$.

Un filtro de Bloom consiste de:

- Un conjunto $\Omega$ de posibles valores (el universo) que pueden aprecer en el flujo
- Un subconjunto $S\subset \Omega$
 de valores que están en la muestra de interés.
- Un vector $v$ de $n$ bits, originalmente igual a 0.
- Una colección de funciones hash $h_1,h_2,\ldots, h_k$ escogidas al azar,
que mapean elementos de $\Omega$ a $n$ cubetas de $1$ a $n$ (posiciones
en el vector de bits).
 
Y el problema consiste en decidir si un elemento 
nuevo $\omega\in \Omega$ está o no en el
conjunto $S$.


### Ejemplo {-}


#### Paso 1: inicialización y selección de hashes {-}

Usaremos un vector de tamaño $n=11$ (longitud de vector de bits), y suponemos
que los valores posibles ($\Omega$) son los enteros de uno a mil. Queremos
detectar cuando observamos algún elemento de $S=\{15,523,922\}$. Para este
ejemplo usamos $k=2$ funciones hash. Estas funciones deben mapear
los enteros del uno al mil a las cubetas 1 a 11 (el número de entradas del
 vector de bits). 

```{r}
# Elementos de la muestra previa 
S <- c(15, 523, 922)

# Funciones has a utilizar con modulo 11 para tener una cubeta por no. de bits. 
hash_f <- list(h_1 = function(x) x %% 11 + 1,
                   h_2 = function(x) (5*x + 3) %% 11 + 1)
```

Inicializamos el vector de bits:

```{r}
# Inicializar vector de bits en ceros. 
v <- integer(11)
```


#### Paso 2: insertar elementos en filtro {-}

Para cada elemento de $S$, calculamos los dos hashes, y ponemos en TRUE
las dos posiciones dadas por estos hashes:

```{r}
# Para cada elemento de la muestra previa 
for(i in 1:length(S)){
  
  # Se obtiene la cubeta a la que se mapea el elemento con cada hash.
  indices <- map_dbl(hash_f, ~ .x(S[i]))
  
  # Imprime las cubetas a las que corresponde 
  print(indices)
  
  # Convierte a TRUE las posiciones ocupadas por los elementos previos.
  v[indices] <- 1
  
  # Imprime el vector de posiciones. 
  print(v)
}
```

Y tenemos el vector del filtro listo:
```{r}
v
```

#### Paso 3: filtrar elementos {-}

Ahora veamos cómo decidimos cuáles elementos están o no en el conjunto $S$. Si
observamos un nuevo número $x$, calculamos sus hashes, y vemos si esas
posiciones están prendidas en el vector $v$. Si al menos una de las dos posiciones no está prendida, entonces el
elemento no está en el conjunto $S$:

```{r}
# Define nuevo elemento. 
z <- 219

# Calcula el valor de las funciones hash. 
h_z <- map_dbl(hash_f, ~.x(z))

# Valor de las funciones hash.
h_z
```

No está en la lista, pues por lo menos uno de los bits es igual a cero:

```{r, warning=FALSE}
# Extrae los bits correspondientes al nuevo elemento. 
v[h_z]

# Revisa si todos los valores son iguales a TRUE. 
all(v[h_z])
```

Para cualquier número en la colección, todas las posiciones de sus hashes tienen
el valor $1$:

```{r, warning=FALSE}
# Número de la muestra 
z <- 523

# Calcula valor de las funciones hash. 
h_z <- map_dbl(hash_f, ~.x(z))

# Revisa si todos los valores son TRUE
all(v[h_z])
```

Sin embargo, puede haber falsos positivos (números que no están en la colección 
$S$ cuyos dos hashes dan posiciones en $1$):

```{r, warning=FALSE}
# Nuevo valor (falso positivo)
z <- 413

# Calcula el valor de las funciones hash. 
h_z <- map_dbl(hash_f, ~.x(z))

# Revisa si todos los valores son TRUE
all(v[h_z])
```

**Observaciones**: 

- Nótese que solo es necesario almacenar el vector de bits
y las funciones hash, y esto generalmente resulta en una representación 
compacta que se puede mantener en memoria.

- La propiedad más importante es que cualquier elemento en la colección 
siempre da un verdadero positivo (detectamos correctamente los que están
en la colección $S$). Es rápido descartar elementos que no están en la 
colección (alguno de los bits de su hash está apagado) - no hay falsos negativos.

- Por otra parte, tendremos algunos falsos positivos, que tenemos
que controlar (probabilidad baja). En la mayoría de los casos, si el filtro
está bien diseñado, cuando intentemos recuperar elementos que dan positivo
en el filtro vamos a encontar el elemento.

- Agregar elementos al filtro y checar los bits para un nuevo elemento
son operaciones relativamente eficientes (depende del número de hashes
y es posible paralelizar).

- No es posible eliminar elementos de un filtro de Bloom. ¿Por qué? Por que si se elimina un elemento se tendría que convertir a cero las cubetas que le corresponden a sus valores hash, pero estos mismos valores hash pueden corresponder a otro elemento de la lista por lo tanto no deben convertirse a cero. Si se desea eliminar un elemento, se debe volver a llenar el filtro desde cero. 

## Análisis de filtro de Bloom


Para construir este filtro, tenemos que escoger el tamaño del vector de bits ($n$),
y el número de funciones hash $k$, dependiendo
del número de elementos que tenemos que almacenar. De estas dos contidades,
y del tamaño del conjunto $S$, depende la tasa de falsos positivos.

Supongamos como aproximación que una función hash, al aplicarla a una $x$ dada,
selecciona
una de las entradas del vector de bits con la misma probabilidad. 
La probabilidad de que un
bit dado no se encienda cuando insertamos un elemento es entonces
$$1-\frac{1}{n}$$.
Si $k$ es el número de funciones hash escogidas independientemente, 
entonces la probabilidad de que ese bit dado no se encienda es
$$\left (1-\frac{1}{n}\right )^k$$.
Si insertamos los $s$ elementos de $S$, la probabilidad de que ese bit dado
no se encienda es entonces
$$\left (1-\frac{1}{n}\right  )^{ks}$$.
La probabilidad de que se encienda es
$$1-\left (1-\frac{1}{n}\right )^{ks}$$.


Finalmente podemos calcular la probabilidad de un falso positivo. Para un elemento
que no está en $S$, la probabilidad de que todos sus hashes caigan en bits encendidos
es

$$ \left ( 1-\left (1-\frac{1}{n}\right )^{ks}\right )^k$$

**Observaciones**:

1. Si usamos un vector más grande ($n$ más grande), la probabildad de falsos
positivos baja (el vector de bits tiene relativamente más ceros).
2. Si el conjunto $S$ es más grande ($s$ más grande), la probabilidad de
falsos positivos sube (el vector de bits está más lleno). Si insertamos 
demasiados elementos en un filtro dado, éste se puede *saturar*, y resultar
en una probabilidad alta de falso positivo.
3. El número de hashes tiene dos efectos: por un lado, más hashes llenan más
el vector de bits de unos. Por otro lado, es más difícil que un nuevo elemento
"atine" a más posiciones que tienen un bit encendido.
4. Esta fórmula es una aproximación, pues usamos funciones hash y no aleatorización.


Podemos hacer una gráfica para ver cómo se comporta la tasa de falsos positivos:


```{r, fig.width = 8}
# Función que calcula la probabilidad de tener un falso positivo. 
tasa_fp <- function(n, s, k) {
    (1 - (1 - (1 / n)) ^ (k * s)) ^ k
}

# Crea posibles combinaciones de una muestra de 100mil,1millon,10millones y 100millones, 
# Hashes de 1 a 20 
# vector de 100mil a mil millones de bytes
df <- expand.grid(list(s = c(1e5, 1e6, 1e7, 1e8), # tamaño de muestra original
        k = seq(1, 20), # hashes
        n = 10 ^ seq(5, 9, by = 0.5))) %>% # bits 
      mutate(mill_bits = round(n/1e6, 1)) %>% # convierte a millones de bits
      mutate(tasa_falsos_p = tasa_fp(n, s, k)) %>% # proba falso positivo
      mutate(s_str = paste0(s, ' insertados')) # modifica s


# Grafica de hashes vs prob fp, color = no bits, facets = insertados. 
ggplot(df, aes(x = k, y = tasa_falsos_p, 
  colour = factor(mill_bits), group = mill_bits)) + 
  geom_line(size = 1.2) +
  facet_wrap(~s_str) +
  labs(x = "k = número de hashes", 
       y =  "Proba de falso positivo",
       colour = "Millones bits \n en vector") +
  scale_y_sqrt(breaks = c(0.01,0.05,0.1,0.25,0.5,1)) 
```

Hay un trade off entre tamaño de muestra original, no de bits, probabilidad de falso positivo y no de hashes. Por ejemplo, para el menor número de insertados (100 mil), podríamos usar una curva de mil millones de bits y parácticamente podríamos asegurar no tener falsos positivos para cualquier no. de hashes pero tener mil millones de bits sería muy costoso. En cambio utilizar 1 millón de bits con aproximadamente 5 funciones hash, nos da una probabilidad de 0.025 de falsos positivos. Para cada una de estas curvas, buscamos el número de hashes que den el menor probabilidad de falso positivo. 


Haciendo algunas aproximaciones, se puede demostrar que el número de hashes
óptimo es aproximadamente
$$k  = \frac{n}{s}\log(2)$$

```{r, warning = FALSE}

# Data frame anterior para calcular k optimo. 
df_opt <- df %>% select(n, s) %>%  
  mutate(k = ceiling((n/s)*log(2))) %>% unique %>%
  mutate(tasa_falsos_p = tasa_fp(n, s, k)) %>%
  mutate(s_str = paste0(s, ' insertados'))

# Grafica de combinaciones de s,k y n junto con aproximación de k óptima. 
ggplot(df, aes(x = k, y = tasa_falsos_p)) +
               geom_line(aes(colour=factor(mill_bits), group=mill_bits),
                 size=1.2) +
               facet_wrap(~s_str) +
               labs(x="k = número de hashes", 
                    y="Probabilidad de falsos positivos",
                    colour = "Mill bits \n en vector") +
               scale_y_sqrt(breaks = c(0.01,0.05,0.1,0.25,0.5,1)) +
               geom_point(data = df_opt, col='red') +
               xlim(0,20)
  
```


```{block2, type='resumen'}
Un filtro de bloom nunca da falsos negativos, pero puede dar falsos positivos.
La tasa de falsos positivos se puede controlar escogiendo el tamaño del vector
y el número adecuado de hashes dependiendo del tamaño esperado del conjunto
que vamos a insertar.
```

## Ejemplo: un corrector de ortografía simple basado en filtro de Bloom

```{r, message = FALSE}
# Leemos las palabras en el diccionario (Español)
diccionario <- read_csv("../datos/diccionario/es_dic.txt", col_names = FALSE) %>% 
          pluck("X1")

# nota: el encoding del diccionario debe ser utf-8
# diccionario <- iconv(diccionario, to = "UTF-8")

# No de palabras en el diccionario. 
m <- length(diccionario)
m
```

Queremos insertar entonces unos 250 mil elementos, aunque puede ser posible
que quizá queramos insertar otras palabras más adelante.

```{r}
# Creamos las posibles combinaciones de s=300mil, k=4:20 y n=1millon a 8millones
df <- expand.grid(list(s = 300000, # valores a insertar
                  k = seq(4, 20), # no. funciones hash
                  n = c(1e6, 2e6, 4e6, 6e6, 8e6) # no bits. 
                  )) %>%
      mutate(millones_bits = (n/1e6)) %>% # convierte a millones 
      mutate(tasa_falsos_p = tasa_fp(n, s, k)) %>% # calcula proba de falsos pos. 
      mutate(s_str = paste0(s, ' insertados'))


# Grafica de análisis de filtro de bloom
ggplot(df, aes(x = k, y = tasa_falsos_p, 
               colour=factor(millones_bits), group=millones_bits)) + 
               geom_line(size=1.2) +
               facet_wrap(~s_str) +
               labs(x="k = número de hashes", 
                    colour = "Mill bits \n en vector") +
               scale_y_log10(breaks= c(0.0001, 0.001, 0.01, 0.1))
```

Podemos intentar usar un vector de 8 millones de bits con unos 6 hashes. Nuestra
estimación de falsos positivos con 6 hashes es de

```{r}
# no de bits 
n <- 8e6

# proba de falsos positivos 
tasa_fp(n = n, s = 300000, k = 6)
```

Ahora necesitamos nuestras funciones hash escogidas al azar. Podemos
usar el algoritmo [xxhash32](https://github.com/Cyan4973/xxHash), por ejemplo:

```{r}
# Carga librerías
library(digest)

# Fija la semilla 
set.seed(18823)

# Genera las k funciones hash con modulo n. 
hash_generator <- function(k = 1, n){
  
  # Fija las semillas. 
  seeds <- sample.int(652346, k)
  
  # Función hash final.
  hasher <- function(x){
    
    # Extrae la semillas
    sapply(seeds, function(s){
      # en digest, serialize puede ser false, pues trabajamos con cadenas
      # la salida de xxhash32 son 8 caracteres hexadecimales, pero 
      # solo tomamos 7 para poder convertir a un entero
      sub_str <- substr(digest::digest(x, "xxhash32",
                                       serialize = FALSE, seed = s), 1, 7)
      
      # convierte a base hexadecimal y luego modulo n
      strtoi(sub_str, base = 16L) %% n + 1
    })
  }
  hasher
}

# Extrae 5 funciones hash. 
hashes <- hash_generator(5, n)  
```

```{r}
# Prueba algunos hashes
hashes('él')
hashes('el')
hashes('árbol')
```


Una implementación del filtro de Bloom es como sigue:


```{r}
# Función para calcular el filtro de bloom. 
# Esta función produce funciones de ella misma. Es una función de tipo constructor. 
# Esta forma particular de escribir funciones hace que no sea necesario copiar 
# cada elemento de R al realizar una operación, por lo tanto ocupa menos memoria y 
# es más rapido.
# Cuando llamo a una función se cambia el valor de v por referencia. 
filtro_bloom <- function(num_hashes, n){
  
    # representación del vector de bits
    v <- raw(n) 
    
    # Genera las funciones hash modulo n
    hashes <- hash_generator(num_hashes, n)
    
    # Insertar palabras del diccionario
    insertar <- function(x){
        x <- iconv(x, "utf-8")
        
        # Nota: esta v es la v del ambiente anterior, lleva la doble flecha para 
        # decirle a R que busque en el ambiente anterior a esta función. 
        # Si pongo la flecha simple va a arrojar un error porque no existe un v en esta función. 
        v[hashes(x)] <<- as.raw(1)
    }
    
    # Función para checar si está en filtro. 
    en_filtro <- function(x){
        
        # Revisa si las posiciones de los hashes dados son todas iguales a uno. 
        all(as.logical(v[hashes(x)]))
    }
    
    # Función para devolver el vector v. 
    vec <- function() v
    
    # Retorno de la función. Notar que todas las salidas refieren al mismo vector v. Cuando acutalizo un valor, se actualiza para todas las salidas. 
    filtro <- list(insertar = insertar, en_filtro = en_filtro, vec = vec)
    
    # Retorno
    filtro
}
```



**Observación**:  Las funciones necesarias (insertar, buscar) son cortas, así
que puedes hacer experimentos utilizándolas directamente en tu código
(sin encapsular en un constructor como la función anterior).

---

Ahora creamos el filtro e insertamos los elementos del diccionario:

```{r, cache = TRUE}
# Fija la semilla 
set.seed(812)

# Crea el filtro de bloom con 6 hashes y 8 millones de bits. 
filtro_b <- filtro_bloom(num_hashes = 6, n = 8e6)

# insertar palabras de diccionario
system.time(
    for(i in seq_along(diccionario)){
        filtro_b$insertar(diccionario[i])
    })
```

El tamaño del filtro es de

```{r}
# Extrae le tamaño del filtro en MB
format(object.size(filtro_b$vec()), units = "Mb")
```

Esto pasa porque no estamos trabajando con bits sino con bytes. La opción para trabajar con bits en R lo que hace es copiar los bits ie no nos sirve de mucho porque es justo lo que queríamos evitar. Otra opción es implementar el código en c++ para tener algo más "profesional". 


Vemos unos ejemplos:

```{r}
# Verificamos si la palabra arbol está en el filtro. 
filtro_b$en_filtro("arbol")
```

Como este valor es falso, la palabra *arbol* definitivamente no está en el diccionario original. 

```{r}
# Verificamos que la palabra árbol esté en el filtro. 
filtro_b$en_filtro("árbol")
```

Por otro lado, la palabra *árbol* prueba positivo. Puede ser un falso positivo, con probabilidad muy baja, como calculamos antes. Podemos calcular la exacta ahora que sabemos
cuántos bits del filtro están ocupados. Primero, la proporción de bits ocupados es:

```{r}
# Recuperamos el vector de bits
v <- as.logical(filtro_b$vec())

# Extraemos el no. de bits. 
n <- length(v)

# Contamos el no. de casillas ocupadas
ocupacion <- sum(as.logical(v))

# Obtenemos la proporción de bits ocupados en el filtro. 
p <- ocupacion / n
p
```

Y la probabilidad de que una palabra que no está en la lista sea un falso positivo
es:

```{r}
# Probabilidad de ser falso positivo.  
p^6
```

Podemos hacer otras pruebas:

```{r}
# Palabras nuevas a verificar si están o no en el filtro. 
palabras_prueba <- c('árbol', 'arbol', 'explicásemos', 'xexplicasemos',
                     'gato', 'perror', 'error', 'perro', 'alluda','ayuda')

# Data frame con las palabras y prueba de si está o no está 
df_palabras <- tibble(palabra = palabras_prueba) %>%
                   mutate(pertenece = map_lgl(palabra, filtro_b$en_filtro))
df_palabras
```

En el siguiente paso tendríamos que producir sugerencias de corrección.
En caso de encontrar una palabra que no está en el diccionario,
podemos producir palabras similares (a cierta distancia de edición),
y filtrar aquellas que pasen el filtro de bloom (ver [How to write a spelling corrector](http://norvig.com/spell-correct.html)).

```{r}

# Busca palabras cercanas haciendo eliminaciones, sustituciones, transposiciones o inserciones. 
generar_dist_1 <- function(palabra){
  
  caracteres <- c(letters, 'á', 'é', 'í', 'ó', 'ú', 'ñ')
  
  # Parte el string en dos partes tomando cada posicion como separador en cada elemento del vector. 
  # Ejem: para la palabra casa tenemos: "" - casa, c-asa, ca-sa, cas-a, casa-""
  pares <- lapply(0:(nchar(palabra)), function(i){
    c(str_sub(palabra, 1, i), str_sub(palabra, i+1, nchar(palabra)))
  })
  
  # palabras obtenidas al eliminar una letra. 
  eliminaciones <- pares %>% map(function(x){ paste0(x[1], str_sub(x[2],2,-1))})
  
  # Palabras obtenidas de sustituir una letra por otra del diccionario
  # (en cada posición) incluyendo acetos y la ñ. 
  sustituciones <- pares %>% map(function(x)
      map(caracteres, function(car){
    paste0(x[1], car, str_sub(x[2], 2 ,-1))
  })) %>% flatten 
  
  # Palabras obtenidas de agregar una letra del diccionario en cada
  # posición incluyendo acentos y la ñ
  inserciones <- pares %>% map(function(x){
    map(caracteres, function(car) paste0(x[1], car, x[2]))
  }) %>% flatten
  
  # Palabras obtenidas de intercambiar dos letras consecutivas 
  transposiciones <- pares %>% map(function(x){
    paste0(x[1], str_sub(x[2],2,2), str_sub(x[2],1,1), str_sub(x[2],3,-1))
  })
  
  # Junta todas las posibles palabras. 
  c(eliminaciones, sustituciones, transposiciones, inserciones) %>% unlist
}
```

```{r}
# Posibles opciones par el caracter perror que estén en el diccionario
# i.e. se generan las posibles palabras y se evaluan con el filtro de bloom 
generar_dist_1('perror') %>% keep(filtro_b$en_filtro)
```

```{r}
# Posibles opciones par el caracter explicassemos que estén en el diccionario
# i.e. se generan las posibles palabras y se evaluan con el filtro de bloom 
generar_dist_1('explicasemos') %>% keep(filtro_b$en_filtro)
```


```{r}
# Posibles opciones par el caracter hayuda que estén en el diccionario
# i.e. se generan las posibles palabras y se evaluan con el filtro de bloom 
generar_dist_1('hayuda') %>% keep(filtro_b$en_filtro)
```

Nota: con estas pruebas podemos ver si dentro de las suegerencias encontramos falsos positivos. De ser así, es señal de que nuestros parámetros $k$ y/o $n$  no son adecuados al problema. 


### Ejercicio {-}

- Encuentra alguna palabra del español que no esté en el filtro (por ejemplo una
de español en México). Agrégala al filtro y verifica que es detectada como
positiva. Busca una posible manera incorrecta de escribirla y prueba la
función de arriba de sugerencias.

- Prueba usando un vector de bits mucho más chico (por ejemplo de 500 mil bits). 
¿Qué tasa de falsos positivos obtienes?

## Muestra distribuida uniformemente en el flujo.

Supongamos que tenemos un histórico de tamaño $n_0$ del flujo de datos. Podemos tomar una muestra
para resumir el flujo. El problema es que cuando llegan nuevos datos, si los incluimos desplazando datos anteriores entonces tendremos sesgo hacia actividad reciente. Una solución es hacer una especie de muestreo de rechazo.

Supongamos entonces que queremos trabajar con una muestra de tamaño aproximado $k$, y que inicialmente
tenemos una muestra uniforme del flujo de tamaño $n_0$.

1. Tomamos una muestra uniforme de tamaño $k$ de los $n_0$ casos.
2. Si observamos un nuevo caso cuando observamos el dato $n > n_0$,
lo seleccionamos con probabilidad $\frac{k}{n}$. 
3. Si el nuevo caso resulta seleccionado, escogemos al azar uno de los $k$ elementos anteriores y lo eliminamos.
4. Repetimos para $n+1$.

Como ejercicio, demostrar:

###  Ejercicio {-}
Al tiempo $n$, la probabilidad de que un elemento del flujo completo 
esté en la muestra es uniforme $k/n$

### Ejemplo {-}
Consideramos $k=100$, y observamos un flujo sintético dado como sigue:

```{r}
# No de observaciones a simular para el flujo total
N <- 100000

# No de observaciones en el flujo histórico
n_0 <- 1000

# Fijar semilla
set.seed(103)

# # ???
# lambda <- 10*abs(sin(1:N/200))

# Simulación del flujo: acumulada de normales. 
datos <- data_frame(n = 1:N, res = rnorm(N)) %>% mutate(obs = cumsum(res))

# Grafica los flujos históricos (menores a n0)
# Los ultimos 50 datos (ventana reciente) se marcan en otro color. 
ggplot(datos %>% filter(n < n_0), aes(x = n, y = obs, colour = factor((n_0 - n) < 50))) +
  geom_line()
```

Si utilizamos una ventana reciente de tamaño 50, nuestras estimaciones del estado del sistema 
están sesgadas a los últimos valores.

Sin embargo, si aplicamos el esquema mostrado arriba:


```{r}
# Calcula la muestra uniforme del flujo original
# Nuevamente tenemos una función de tipo constructor. 
muestra_unif <- function(datos_ini, k){
  
  # Tamaño del flujo original
  n <- length(datos_ini)
  
  # Muestra de tamaño k
  muestra <- sample(datos_ini, k)
  
  # Función para seleccionar un dato.
  seleccion <- function(dato){
    
    # Reemplaza el tamaño de muestra por n+1
    n <<- n + 1
    
    # Obtiene un valor de una uniforme 0-1
    if(runif(1) < k/n) {
      
      # si es menor que k/n se selecciona el dato para entrar a la muestra. 
      # Aleatoriamente reemplaza a un elemento. 
      muestra[sample.int(k, 1)] <<- dato
    }
    
    # Calcula la media de la muestra
    mean(muestra)
  }
  
  # Función para calcular la media de la muestra. 
  media <- function(){
    
      mean(muestra)
  }
  
  # Lista con las funciones de salida.
  list(seleccion = seleccion, media = media)
  
}

# Fija la semilla 
set.seed(8128324)

# Calcula una muestra aleatoria de los primeros 100 datos de tamaño 50.
muestra_u <- muestra_unif(datos_ini = datos$obs[1:100], k = 50)
muestra_u$media()
```

Ahora observamos el flujo y vamos agregando y rechazando:

```{r}
# Para las observaciones 101 a 5000, se va  agregando o rechazando cada observación 
datos_p <- datos %>% filter(n >= 101, n < 5000) %>%
  mutate(promedio_muestra = map_dbl(obs, muestra_u$seleccion))
```

Y observamos que nuestro esquema da una buena estimación de la media
total para cada $n$:

```{r}
datos_p %>% # datos
  mutate(promedio_total = cummean(obs)) %>% # media acumulada del valor observado
  gather(variable, valor, obs:promedio_total) %>% # formato long
  ggplot(aes(x = n, y = valor, colour = variable)) + geom_line() #gráfica
```

## Contando elementos diferentes en un flujo.

Supongamos que queremos contar el número de elementos diferentes que aparecen 
en un flujo, por ejemplo, cuántos usuarios únicos tiene un sitio (según
un identificador como login o ip, por ejemplo).

Como antes, si el número de elementos distintos no es muy grande, podemos
usar una estructura eficiente en memoria (como un diccionario o tabla hash) para procesar
cada elemento nuevo del flujo, decidir si es nuevo, agregarlo a la estructura,
y contar. Sin embargo, si el número de elementos distintos es grande, esta
estrategia puede requierir mucha memoria y procesamiento. Un filtro de bloom 
no es del todo adecuado, pues confrome vayamos
llenando de 1´s el su vector, la tasa de falsos positivos irá incrementando (en el 
límite es 1).

Una alternativa es usar algoritmos probabilísticos, que utilicen mucha menos
memoria, siempre y cuando aceptemos cierto error de estimación.

### El algoritmo de Flajolet-Martin

Este es uno de los primeros algoritmos para atacar este problema, y se basa
en el uso de funciones hash. La referencia básica es este (paper)[http://algo.inria.fr/flajolet/Publications/FlFuGaMe07.pdf], [@Flajolet]

```{block2, type='resumen'}
La idea básica del algoritmo de Flajolet-Martin se basa en la siguiente observación:

  Si escogemos funciones hash que mapeen elementos del conjunto del flujo a 
una sucesión de bits suficientemente grande, conforme haya más elementos distintos
en el flujo observaremos más valores hash distintos, y en consecuencia, es más
probable observar sucesiones de bits con características especiales.

La característica especial que se explota en este algoritmo es el número
de ceros que hay al final de las cadenas de bits.
```


### Ejemplo {-}
Consideramos una función hash (para cadenas):
```{r}

# Función hash para cadenas
hash_gen <- function(seed){
  
  # Función hash para la semilla específica
  function(x){

    # hash_32 <- digest::digest(x, 'xxhash32', serialize = FALSE, seed = seed) 
    # Covertimos a bits, tomando de dos en dos:
    # Esta implementación es lenta
    #sapply(seq(1, nchar(hash_32), 2), function(x) substr(hash_32, x, x+1)) %>%
    #    strtoi(16L) %>% as.raw %>% rawToBits()
    # versión más rápida con hash_string
    textreuse::hash_string(x) %>% bitwXor(seed) %>% intToBits()
  }
}

# Fija la semilla 
set.seed(5451)


# # Genera las funciones hash 
# hash_1 <- hash_gen(seed = 123)
# hash_2 <- hash_gen(seed = 564)
# hash_1("7yya4071872aa")

hash_1 <- hash_gen(seed = 123332233)
hash_2 <- hash_gen(seed = 56889004)
hash_1("7yya4071872dfdfdfdfaa")
```

Y ahora hacemos una función para contar el número 0's consecutivos
en la cola de esta representación:

```{r}
# Calcula el no. de ceros al inicio de cada cadena de bits. 
tail_length <- function(bits){
  bits %>% which.max - 1  
}

# Calcula el no. de ceros para el string ejemplo
hash_1("7yya4071872aa") %>% tail_length
```

La idea es que conforme veamos más elementos distintos, es más probable observar
que la cola de ceros es un número más grande. Como la función hash que usamos
es determinista, los elementos ya vistos no contribuyen a hacer crecer a este número.

### Discusión {-}

Antes se seguir, hacemos la siguiente observación: Si consideramos
los bits de cada nuevo elemento como aleatorios: 

- La probabilidad de
que observemos una cola de 0's de tamaño **al menos** $m$ es $2^{-m}$, para $m \geq 1$ 

- Supongamos
que tenemos una sucesión de $n$ candidatos del flujo distintos. La probabilidad de
que *ninguno* tenga una cola de ceros de tamaño mayor a $m$ es igual a
\begin{equation}
(1-2^{-m})^{n}
(\#eq:probacola)
\end{equation}

Que también es la probabilidad de que el máximo de las colas sea menor
a $m$. Reescribimos como


$$((1-2^{-m})^{2^m})^{\frac{n}{2^{m}}}. $$


Ahora notamos que la expresión de adentro se escribe (si $m$ no es muy chica) como
$$P(max < m) = (1-2^{-m})^{2^m} = (1-1/t)^t\approx e^{-1}\approx 0.3678$$ 

- Si $n$ es mucho más grande que $2^m$, entonces la expresión \@ref(eq:probacola) es chica,
y tiende a $0$ conforme $n$ crece.
- Si $2^m$ es mucho más grande que $n$, entonces la expresión \@ref(eq:probacola) es cercana
a $1$, y tiende a $1$ conforme $m$ crece.

- Así que para una sucesión de $n$ elementos distintos, es poco probable observar que
la longitud $m$ de la máxima cola de 0's consecutivos es tal que $2^m$ es mucho más grande que $n$ o mucho más chica que $n$. Abajo graficamos unos ejemplos:

```{r}

# Calcular que la probabilidad de que el valor máximo de la cola de ceros sea r
proba_cola <- function(distintos, r){
  
  # proba de que el max de las colas sea menor a r
  # corresponde a la ecuacion 4.1
  al_menos_r <- 1- (1-0.5^r) ^ distintos
  
  # Probabilidad de que el max de las colas sea mayor a r 
  no_mas_de_r <- 1 - (1-0.5 ^ {r+1}) ^ distintos
  
  # Proba de que sea de tamaño r.
  prob <-  al_menos_r - no_mas_de_r 
  prob
}

# Generar data frame para distintos no. de elementos
df_prob <- data_frame(n = c(2^5, 2^10, 2^20)) %>% # 2 a la num de elementos distintos. 
  mutate(probas = map(n, function(n){ 
    m <- 1:30 # tamaño de la cola 
    probas <- sapply(m, function(x){proba_cola(n, x)}) #calcula la probab de ser de tamaño m 
    tibble(m = m, probas = probas)
    })) %>%
  unnest

# Grafica de comparaciones 
ggplot(df_prob, aes(x = 2^m, y = probas, colour = factor(n))) + geom_line() +
   ylab("Probabilidad de máxima cola de 0s") +
    scale_x_log10(breaks=10 ^ (1:7))

# c(2^5, 2^10, 2^20)
```

Usamos $2^m$ para estimar $n$ pues los máximos se dan cuando se satisface $2^m\approxn$. 
---

Y ahora podemos probar cómo se ve la aproximación con dos funciones
hash diferentes:


```{r}
# no. de candidatoas del flujo distintos
n <- 1000

# Función hash 1: composición del hash 1 y la cuenta de ceros en colas. 
tail_hash_1 <- compose(tail_length, hash_1)

# Función hash 2: composición del hash 2 y la cuenta de ceros en colas. 
tail_hash_2 <- compose(tail_length, hash_2)

# Datos 
df <- data_frame(num_distintos = 1:n) %>% # no de candidatos
      mutate(id = as.character(sample.int(52345678, n))) %>% # id del candidato 
      mutate(tail_1 = map_dbl(id, tail_hash_1)) %>% # no de ceros en la cola con el hash1
      mutate(tail_2 = map_dbl(id, tail_hash_2)) # no de ceros en la cola con el hash2

# datos 
df      
```

Y ahora calculamos el máximo acumulado

```{r}
# Calculamos el máximo acumulado para cada hash
df <- df %>% mutate(max_tail_1 = cummax(tail_1), max_tail_2 = cummax(tail_2))

#Vemos los máximos acumulados
tail(df)
```

```{r}
# Grafica hash 1 en escala logarítmica
ggplot(df, aes(x = num_distintos, y = 2^max_tail_1)) + 
  geom_abline(slope=1, intercept = 0, colour = "red") + 
  geom_point() +
  scale_x_log10() + scale_y_log10()
```

```{r}
# Grafica hash 2 en escala logarítmica
ggplot(df, aes(x = num_distintos, y = 2^max_tail_2)) + 
  geom_abline(slope=1, intercept = 0, colour = "red") + 
  geom_point() +
  scale_x_log10() + scale_y_log10()
```

Nótese que las gráficas están en escala logarítmica, así que la estimación 
no es muy buena en términos absolutos si usamos un solo hash. Sin embargo, 
confirmamos que la longitud máxima de las colas de 0's crece con el número
de elementos distintos en el flujo.

## Combinación de estimadores, Hyperloglog

Como vimos en los ejemplos anteriores, la estimación de Flajolet-Martin
tiene dos debilidades: varianza alta, y el hecho de que el único resultado
que puede dar es una potencia de 2.

Podemos usar varias funciones hash y combinarlas de distintas maneras
para obtener una mejor estimación con menos varianza. 

- La primera idea, que puede ser promediar los valores obtenidos de varias
funciones hash, requeriría muchas funciones hash por la varianza alta del estimador, 
de modo que esta opción no es muy buena.
En nuestro ejemplo anterior, la desviación estándar del estimador es:

```{r}
df_prob %>% group_by(n) %>% # agrupa por no. de obs posibles
  mutate(media = sum((2^m)*probas)) %>% # calcula la media 
  summarise(desv_est = sqrt(sum(probas*(2^m-media)^2))) # calcula la ds
```

- Usar la mediana para evitar la posible variación grande de este estimador tiene
la desventaja de que al final obtenemos una estimación de la forma $2^R$, que también
tiene error grande.

- Una mejor alternativa es utilizar la recomendación de [@mmd], que consiste
en agrupar en algunas cubetas las funciones hash, promediar los estimadores $2^{R_i}$
dentro de cada cubeta, y luego obtener la mediana de las cubetas.

### Hyperloglog

Esta solución (referida en el paper anterior, [@Flajolet]) es una de las más utilizadas y refinadas.
En primer lugar:

- Para hacer las cubetas usamos los mismos bits producidos por el hash (por ejemplo,
los primeros $p$ bits). Usamos los últimos bits del mismo hash para calcular la longitud
de las colas de 0's.
- Usamos promedio armónico de los valores máximos de cada cubeta (más robusto
a valores grandes y atípicos, igual que la media geométrica).
- Intuitivamente, cuando dividimos en $m$ cubetas un flujo de $n$ elementos, cada flujo
tiene aproximadamente $n/m$ elementos. Como vimos arriba, lo más probable
es que la cola máxima en cada cubeta sea aproximadamente $\log_2(n/m)$. El promedio
armónico $a$ de $m$ cantidades $(n/m)$ de esta cantidad entonces debería estar ser
del orden en $n/m$, así que la estimación final de la cardinalidad del flujo
completo es $ma$ (el número de cubetas multiplicado por el promedio armónico). 
- Existen varias correcciones adicionales para mejorar su error en distintas circunstancias (dependiendo del número de elemntos únicos que estamos contando, por ejemplo). Una típica
es multiplicar por 0.72 el resultado de los cálculos anteriores para corregir sesgo
multiplicativo (ver referencia de Flajolet).

Veamos una implementación **simplificada** (nota: considerar *spark* para hacer
esto, que incluye una implementación rápida del hyperloglog), usando las funciones hash que construimos arriba.

Primero construimos la función que separa en cubetas, y una nueva
función para calcular la longitud de la cola una vez que quitamos los bits
que indican la cubeta:

```{r}

# Se fija el no de bits para construir las cubetas
cubeta_bits <- 5

# Se calcula m
m <- 2^cubeta_bits

# Tamaño de cola ceros al eliminar los primeros bits. 
tail_length_lead <- function(bits){
  bits[-c(1:cubeta_bits)] %>% which.max %>% as.integer
}

# String de ejemplo
hash_1("7yya40787")

# Tamaño de la cola sin los 5 primeros bits
hash_1("7yya40787") %>% tail_length_lead

# Calcula la cubeta con los primeros bits
cubeta <- function(bits){
  paste0(as.character(bits[1:cubeta_bits]), collapse = "")
}

# Calcula la cubeta para el ejemplo 
hash_1("7yya40787") %>% cubeta
```

Simulamos unos datos y calculamos la cubeta para cada dato:

```{r}

# No. elems distintos (100 mil)
n <- 100000


# # Se fija el no de bits para constuir las cubetas
# cubeta_bits <- 5
# 
# # Datos simulados 
# df <- data_frame(num_distintos = 1:n) %>% # elems distintos
#       mutate(id = as.character(sample.int(52345678, n, replace = FALSE))) %>% # muestreo de datos sin reemplazo
#       mutate(hash = map(id, hash_1)) %>% # aplica fn hash 1
#       mutate(cubeta = map_chr(hash, cubeta)) # Calcula cubeta

hash_1 <- compose(intToBits, textreuse::hash_string)
df <- data_frame(num_distintos = 1:n) %>%
      mutate(id = as.character(sample.int(52345678, n, replace = FALSE))) %>%
      mutate(hash = map(id, hash_1)) %>%
      mutate(cubeta = map_chr(hash, cubeta))

df
```

Y calculamos la longitud de la cola:

```{r}
# Longitud de la cola sin los primeros bits 
df <- df %>% mutate(tail = map_int(hash, tail_length_lead))
df      
```

Ahora vemos cómo calcular nuestra estimación. cuando hay 50 mil distintos, calculamos
máximo por cubeta

```{r}
# Para 50mil elementos distintos
resumen_50 <- df %>% # datos 
  filter(num_distintos <= 50000) %>% # filtra obs
    group_by(cubeta) %>% # agrupa por cubeta
    summarise(tail_max = max(tail)) # obtiene el max de la cola

# Resumen 
resumen_50
```

Y luego calculamos la media armónica y reescalamos para obtener:

```{r}
# Función para calcular la media armónica
armonica <- function(x) 1/mean(1/x)

# Media armónica reescalada para 
0.72 * m * armonica(2 ^ resumen_50$tail_max)

```

Y esta es nuestra estimación de únicos en el momento que el verdadero valor
es igual a 50000.

Podemos ver cómo se desempeña la estimación conforme nuevos únicos van llegando (el 
siguiente cálculo son necesarias algunas manipulaciones para poder calcular
el estado del estimador a cada momento);

```{r}
# Copleta las posibles combinaciones de cubeta y tamaño de cola
res <- df %>% spread(cubeta, tail, fill = 0) %>% # convierte a wide por cubeta llenando el tamaño de cola maxima. 
        gather(cubeta, tail, -num_distintos, -id, -hash) %>% # conviertea long eliminando distintos, id y hash
        select(num_distintos, cubeta, tail)  # selecciona columnas

# Agrupar por cubeta
res_2 <- res %>% 
      group_by(cubeta) %>% # agrupa por cubeta 
      arrange(num_distintos) %>% # ordena por num_distintos
      mutate(tail_max = cummax(tail)) %>% # maximo acumulado
      group_by(num_distintos) %>% # agrupa por num_distintos
      summarise(estimador_hll = 0.72*(m*armonica(2^tail_max))) # estima no distintos. 


# grafica estimadores en cada momento
ggplot(res_2 %>% filter(num_distintos > 100),
       aes(x = num_distintos, y = estimador_hll)) + geom_line() +
  geom_abline(slope = 1, colour ='red') 
```


Finalmente, examinamos el error relativo:

```{r}
# Calculo de cuantiles del error 
quantile(1 - res_2$estimador_hll/res_2$num_distintos, probs=c(0.1, 0.5, .9))
```


**Observaciones**
- Ver también [este paper](https://stefanheule.com/papers/edbt13-hyperloglog.pdf) para mejoras del hyperloglog (por ejemplo, si es posible es preferible usar
hashes de 64 bits en lugar de 32).

- El error relativo teórico del algoritmo (con algunas mejoras que puedes ver en los papers citados) es de $1.04/\sqrt{m}$, donde $m$ es el número de cubetas, así que más cubetas mejoran el desempeño.

- Las operaciones necearias son: aplicar la función hash, calcular cubeta, y actualizar
el máximo de las cubetas. La única estructura que es necesario mantener es
los máximos de las colas dentro de cada cubeta que se actualiza secuencialmente.

### Implementación de spark 

La implementación de hyperloglog en Spark se puede utilizar con el siguiente código:

```{r}
# Carga librería sparklyr
library(sparklyr)

# Conección a sparklyr
sc <- spark_connect(master = "local") # esto normalmente no lo hacemos desde R

# Copiar tabla de datos 
df_tbl <- copy_to(sc, df %>% select(num_distintos, id))

# Aproximación de no. distintos. 
df_tbl %>%
  summarise(unicos_hll = approx_count_distinct(id)) # error estándar relativo 0.05 

```

## Tarea{-}

- Resuelve los dos ejercicios pendientes (uno en filtro de Bloom, otro en la sección de muestras uniformemente distribuidas.

- Repetir la estimación del hyperloglog del ejemplo de clase aumentando a 250-500 mil elementos distintos. Puedes utilizar la implementación de spark. ¿Qué errror relativo

obtuviste? Nota: puedes también usar la implementación en R, pero es considerablemente
más lenta que la versión de Spark

