#Similitud y minhashing


En la primera parte del curso trataremos un problema fundamental en varias tareas de minería de datos: ¿cómo medir similitud, y cómo encontrar vecinos cercanos en un conjunto de datos?

Algunos ejemplos son:

- Encontrar documentos similares en una colección de documentos (este es el que vamos a tratar más). Esto puede servir para detectar
plagio, deduplicar noticias o páginas web, etc. Ver por ejemplo [Google News]((https://dl.acm.org/citation.cfm?id=1242610)).
- Encontrar imágenes similares en una colección grande, ver por ejemplo [Pinterest](https://medium.com/@Pinterest_Engineering/detecting-image-similarity-using-spark-lsh-and-tensorflow-618636afc939).
- Encontrar usuarios similares (Netflix), en el sentido de que tienen gustos similares. O películas similares, en el sentido de que le gustan a las mismas personas.
- Uber: rutas similares que indican fraude o abusos [https://eng.uber.com/lsh/].
- Deduplicar registros de usuarios de algún servicio (por ejemplo, beneficiarios
de programas sociales).

Estos problemas no son triviales por dos razones:

- Los elementos que queremos comparar muchas veces están naturalmente representados en espacios de dimensión alta, y es relativamente costoso comparar un par (documentos, imágenes, usuarios, rutas). Muchas veces es preferible construir una representación más compacta y hacer comparaciones con las versiones comprimidas.
- Si la colección de elementos es grande ($N$), entonces el número de pares 
posibles es del orden de $N^2$, y no es posible hacer todas las posibles comparaciones para encontrar los elementos similares (por ejemplo, comparar
$100$ mil documentos, con unas $10$ mil comparaciones por segundo, tardaría alrededor de $5$ días).

Si tenemos que calcular *todas* las similitudes, no hay mucho qué hacer. Pero
muchas veces nos interesa encontrar pares de similitud alta, o completar tareas
más específicas como contar duplicados, etc. En estos casos, veremos que es
posible construir soluciones probabilísticas aproximadas para resolver estos
problemas de forma escalable. 

## Similitud de conjuntos

Muchos de estos problemas de similitud se pueden pensar como 
problemas de similitud entre conjuntos. Por ejemplo, los documentos son conjuntos de palabras, pares de palabras, sucesiones de caracteres,
una película como el conjunto de personas a las que le gustó, o una ruta
como un conjunto de tramos, etc.

Hay muchas medidas que son útiles para cuantificar la similitud entre conjuntos. Una que es popular, y que explotaremos por sus propiedades, es la similitud de Jaccard:


```{block2, type='resumen'}
La **similitud de Jaccard** de los conjuntos $A$ y $B$ está dada por

$$sim(A,B) = \frac{|A\cap B|}{|A\cup B|}$$

```

Esta medida cuantifica qué tan cerca está la unión de $A$ y $B$ de su intersección. Cuanto más parecidos sean $A\cup B$ y $A\cap B$, más similares son los conjuntos ($sim(A,B)\approx 1$), cuanto más diferentes sean $A\cup B$ y $A\cap B$, más diferentes son los conjuntos ($sim(A,B)\approx 0$). En términos geométricos, es el área de la intersección entre el área de la unión.

#### Ejercicio {-}
Calcula la similitud de Jaccard entre los conjuntos $A=\{5,2,34,1,20,3,4\}$
 y $B=\{19,1,2,5\}$
 
**Solución:**

$A\cup B =\lbrace 1,2,3,4,5,19,20,34\rbrace$

$A\cap B=\lbrace 1,2,5\rbrace$

Entonces: $|A\cup B|=8$ y $|A\cap B|=3$, por lo tanto 

$$
sim(A,B)= \dfrac{|A\cap B|}{|A\cup B|}=\dfrac{3}{8}=0.375
$$

Otros ejemplos: 

```{r, collapse = TRUE, warning=FALSE, message=FALSE}
# Cargamos la libreria
library(tidyverse)

# Función para calcular la similitud de Jaccard 
sim_jaccard <- function(a, b){
  
  # Tamaño de la intersescción entre tamaño de la unión. 
    length(intersect(a, b)) / length(union(a, b))
}

# Ejemplo 1: 
sim_jaccard(c(0,1,2,5,8), c(1,2,5,8,9))

# Ejemplo 2: 
sim_jaccard(c(2,3,5,8,10), c(1,8,9,10))

# Ejemplo 3: 
sim_jaccard(c(3,2,5), c(8,9,1,10))
```


## Representación en tejas para documentos

En primer lugar, buscamos representaciones
de documentos como conjuntos. Hay varias maneras de hacer esto. 

Consideremos una colección de textos cortos:

```{r}
# Vector de caracteres de tamaño 4. 
textos <- character(4)

# Ingresamos el valor del documento. 
textos[1] <- 'el perro persigue al gato.'
textos[2] <- 'el gato persigue al perro'
textos[3] <- 'este es el documento de ejemplo'
textos[4] <- 'el documento con la historia del perro y el gato'
```

Los métodos que veremos aquí se aplican para varias representaciones:

- La representación
más simple es la bolsa de palabras, que es conjunto de palabras que contiene un
documento. Podríamos comparar entonces documentos calculando la similitud de Jaccard 
de sus bolsas de palabras (1-gramas)

```{r}
# Extraemos las palabras en el texto 1 de una en una
tokenizers::tokenize_words(textos[1])
```

- Podemos generalizar esta idea y pensar en $n$-gramas de palabras, que son sucesiones
de $n$ palabras que ocurren en un documento.

```{r}
# Extraemos bigramass (conjuntos de 2 palabras) en una ventana móvil. 
tokenizers::tokenize_ngrams(textos[1], n = 2)
```


- Otro camino es usar $k$-tejas, que son $k$-gramas de *caracteres*

```{r, collapse= TRUE}
# Esta es una implementación simple para extraer tejas
shingle_chars_simple <- function(string, lowercase = FALSE, k = 4){
  
    # produce tejas (con repeticiones)
    if(lowercase) {
      
      # convierte a minúculas. 
      string <- str_to_lower(string)
    }
  
    # Crea vector vacío de posibles tejas 
    # extrae caracteres de tamaño k en ventana móvil 
    shingles <- seq(1, nchar(string) - k + 1) %>%
        map_chr(function(x) substr(string, x, x + k - 1))
    
    # Devuelve tejas
    shingles
}

# Veamos un ejemplo 
ejemplo <-shingle_chars_simple('Este es un ejemplo')
ejemplo

# Preferimos la del paquete tokenizers
shingle_chars <- function(string, k, lowercase = FALSE){
  
  # Función para extraer shingles del paquete tokenizers. 
    tokenizers::tokenize_character_shingles(string, n = k, lowercase = FALSE,
        simplify = TRUE, strip_non_alphanum = FALSE)
}

# Veamos el mismo ejemplo. 
ejemplo <- shingle_chars('Este es un ejemplo', 4)
ejemplo
```

Si lo que nos interesa principalmente es la similitud textual (no significado, o polaridad, etc.) entre documentos, entonces podemos comparar dos documentos considerando que sucesiones de caracteres de tamaño fijo ocurren en ambos documentos, usando $k$-tejas. Esta
representación es **flexible** en el sentido de que se puede adaptar para documentos muy cortos (mensajes o tweets, por ejemplo), pero también para documentos más grandes.


```{block2, type = 'resumen'}
**Tejas (shingles)**
  
Sea $k>0$ un entero. Las $k$-tejas ($k$-shingles) de un documento d
 es el conjunto de todas las corridas (distintas) de $k$
caracteres sucesivos.

```

Es importante escoger $k$ suficientemente grande, de forma que la probabilidad de que
una teja particular ocurra en un texto dado sea relativamente baja. Si los textos
son cortos, entonces basta tomar valores como $k=4,5$, pues hay un total de $27^4$ tejas

de tamaño $4$, y el número de tejas de un documento corto (mensajes, tweets) es mucho más bajo que
$27^4$ Nota: Este argumento no es exactamente correcto, pues shay tejas que nunca van a aparecer en el 
español (por ejemplo "xya "); entonces, el valor $27^4$ está sobrestimando el no. total de tejas posibles. 

Para documentos grandes, como noticias o artículos, es mejor escoger un tamaño más grande,
como $k=9,10$, pues en documentos largos puede haber cientos de miles
de caracteres, si $k$ fuera más chica entonces una gran parte de las tejas aparecería en muchos de los documentos.

#### Ejemplo {-}

Documentos textualmente similares tienen tejas similares:

```{r, collapse = TRUE}
# Vector de caracteres para ingresar los documentos. 
textos <- character(4)

# LLenamos los documentos. 
textos[1] <- 'el perro persigue al gato, pero no lo alcanza'
textos[2] <- 'el gato persigue al perro, pero no lo alcanza'
textos[3] <- 'este es el documento de ejemplo'
textos[4] <- 'el documento habla de perros, gatos, y otros animales'

# extraemos las tejas en cada documento. 
tejas_doc <- map(textos, shingle_chars, k = 4)

# Calculamos la similitud de jaccard para las combinaciones de posibles 
# de pares de documentos 4!/(2!2!)=24/4=6 
sim_jaccard_result<-data_frame(Docs=paste("Doc",c(1,1,1,2,2,3),"vsDoc",c(2,3,4,3,4,4),sep=''),
                               Sim_Jaccard=c(sim_jaccard(tejas_doc[[1]], tejas_doc[[2]]),
                                             sim_jaccard(tejas_doc[[1]], tejas_doc[[3]]),
                                             sim_jaccard(tejas_doc[[1]], tejas_doc[[4]]),
                                             sim_jaccard(tejas_doc[[2]], tejas_doc[[3]]),
                                             sim_jaccard(tejas_doc[[2]], tejas_doc[[4]]),
                                             sim_jaccard(tejas_doc[[3]], tejas_doc[[4]])))

# Comparación de resultados. 
DT::datatable(sim_jaccard_result%>%
    mutate_if(is.numeric, funs(round(., 3))))
```

*Observación*: las $n$-tejas de palabras se llaman usualmente $n$-gramas. Lo
que veremos aquí aplica para estos dos casos.


## Reducción probablística de dimensión

La representación usual de $k$-tejas de documentos es una representación de dimensión alta: tenemos un vector tantas entradas como tejas, y cada entrada indica si la
teja está o no en el documento:

```{r}
# Extraemos todas las tejas que aparecen en los documentos. 
todas_tejas <- unlist(tejas_doc) %>% unique %>% sort

# Convertimos a 0 y 1's las tejas del documento 1
vector_doc_1 <- as.numeric(todas_tejas %in% tejas_doc[[1]])

# Asignamos nombre al vector de tejas
names(vector_doc_1) <- todas_tejas

# Vemos el vector de tejas del doc. 1
vector_doc_1
```

Para esta colección chica, con $k$ relativamente pequeño, el vector
que usamos para representar cada documento es de tamaño `r length(vector_doc_1)`,
pero en otros casos este número será mucho más grande. 

Podemos construir expícitamente la matriz de tejas-documentos de las siguiente forma (OJO: esto normalmente **no** queremos hacerlo, pero lo hacemos para ilustrar):


```{r}
# Data frame con Id de documento y las tejas correspondientes
df <- data_frame(id_doc = paste0('doc_',seq(1, length(tejas_doc))), 
                 tejas = tejas_doc) %>%  # matriz de tejas por documentos
        unnest %>% # se confierte a dataframe
        unique %>% # se eliminan repetidos
        mutate(val = 1) %>% # se asignan 1's a las tejas observadas
        spread(id_doc, val, fill = 0) # Se convierte a formato short con 0 para las tejas no obs. 

# Vemos el dataframe
df
```

¿Cómo calculamos la similitud de Jaccard usando estos datos?

Calcular la unión e intersección se puede hacer haciendo OR y AND de las columnas, y
entonces podemos calcular la similitud
```{r}
# Calcula intersección de doc. 1 y 2
# Si doc1=doc2=1 -> 1, Si doc1=doc2=0->0, eoc->0
inter_12 <- sum(df$doc_1 & df$doc_2)

# Calcula union de doc 1 y 2
# Si doc1=doc2=1 -> 1, si doc1=1, doc2=0 -> 1, si doc1=0, doc2=1 -> 1, eco -> 0
union_12 <- sum(df$doc_1 | df$doc_2)

# Calcula la similitud de Jaccard
similitud <- inter_12/union_12

# Comparar con el número que obtuvimos arriba.
similitud 
```
Obtenemos el mismo resultado que lo calculado antes. 


Ahora consideramos una manera probabilística de reducir la
dimensión de esta matriz sin perder información útil para
calcular similitud. Queremos obtener una matriz con menos renglones
(menor dimensión) y las mismas columnas (documentos).

Los mapeos que usaremos son escogidos al azar, y son sobre
el espacio de enteros.

- Sea $\pi$ una permutación al azar de los renglones de la matriz.
- Permutamos los renglones de la matriz tejas-documentos según $\pi$.
- Definimos una nuevo descriptor, el **minhash** del documento: para cada documento (columna) $d$ de la matriz permutada, tomamos el entero $f_\pi (d)$ que da el 
número del primer renglón que es distinto de $0$.

#### Ejercicio {#ej1}

Considera la matriz de tejas-documentos para cuatro documentos y cinco tejas
dada a continuación, con las permutaciones $(2,3,4,5,1)$ (indica que el renglón
$1$ va al $2$, el $5$ al $1$, etc.) y $(2,5,3,1,4)$. Calcula el descriptor definido arriba.

```{r, echo = FALSE}
mat <- matrix(c(c(1,0,0,1), c(0,0,1,0), 
            c(0,1,0,1), c(1,0,1,1),
            c(0,0,1,0)), nrow = 5, ncol = 4, byrow = TRUE)
colnames(mat) <- c('d_1','d_2','d_3','d_4')
rownames(mat) <- c('abc', 'ab ','xyz','abx','abd')
mat
```

**Ver mis notas a mano para la solución del ejercicio.**


#### Ejemplo {-}

Ahora regresamos a nuestro ejemplo de $4$ textos chicos.
Por ejemplo, para una permutación tomada al azar:

```{r}
# Fijamos la semilla
set.seed(321)

# Permutamos los renglones de la matriz de tejas por documento. 
# Nota: sample hace una selección aleatoria de tamaño n de las filas de una tabla.  
df_1 <- df %>% sample_n(nrow(df))

# Vemos la matriz. 
df_1
```

Los minhashes (firmas) para cada documento con esta permutación es:

```{r}
# Extrae los minhashes ie la posición del primer 1. 
df_1 %>% summarise_at(vars(matches('doc')), detect_index, ~.x == 1) 
```

Ahora repetimos con otras permutaciones:

```{r}
# Función para calcular firmas de permutaciones. 
calc_firmas_perm <- function(df, permutaciones){
    # calcula los minhashes para cada permutación dada.  
    map(permutaciones, function(pi){
      
      # Ordena renglones según la permutación pi. 
      df_1 <- df[order(pi), ]
      
      # Extrae los minhasshes de cada documento,
      # ie. encuentra el primer 1 en cada columna
      firma <- df_1 %>% summarise_at(vars(matches('doc')), detect_index, ~.x == 1)
      
      # Devuelve la firma 
      firma
      
    }) %>%
    
    bind_rows %>% # junta las firmas obtenidas por cada permutación. 
    
    add_column(firma = paste0('h_', 1:length(permutaciones)), .before = 1) # Agrega indice de permutación. 
}

# Fija la semilla. 
set.seed(32)

# Define el no. de hashes (no permutaciones) a usar. 
num_hashes <- 12

# Obtiene un total de num_hashhes permutaciones para la matriz de tejas.
# Todas tienen tamaño nrow(df)
permutaciones <- map(as.integer(1:num_hashes), ~ sample.int(n = nrow(df)))

# Calcula la firma para cada una de las permutaciones y las pone en un nvo dataframe. 
t1<-Sys.time()
firmas_perms <- calc_firmas_perm(df, permutaciones)
t1<-Sys.time()-t1

# Matriz de firmas 
firmas_perms
```



---

A esta nueva matriz le llamamos **matriz de firmas** de los documentos.  La firma de un documento es una sucesión de enteros.

Cada documento se describe ahora con `r nrow(firmas_perms)` entradas,
en lugar de `r nrow(df_1)`.

Nótese que por construcción, cuando dos documentos son muy similares,
es natural que sus columnas en la matriz de firmas sean similares, pues la mayor parte
de los renglones de estos dos documentos son $(0,0)$ y $(1,1)$.
Resulta ser que podemos cuantificar esta probabilidad. Tenemos el siguiente
resultado simple pero sorprendente:

```{block2, type = 'resumen'}
Sea $\pi$ una permutación escogida al azar, y $a$ y $b$ dos columnas
dadas. Entonces
$$P(f_\pi(a) = f_\pi(b)) = sim(a, b)$$
donde $sim$ es la similitud de Jaccard basada en las tejas usadas.

Sean $\pi_1, \pi_2, \ldots \pi_n$ permutaciones escogidas al azar de
manera independiente. Si $n$ es grande, entonces por la ley de los grandes números
$$sim(a,b) \approx \frac{|\pi_j : f_{\pi_j}(a) = f_{\pi_j}(b)|}{n},$$
es decir, la similitud de Jaccard es aproximadamente la proporción 
de elementos de las firmas que coinciden.
```


### Ejemplo {-}

Antes de hacer la demostración, veamos como aplicaríamos a la matriz
de firmas que calculamos arriba. Tendríamos, por ejemplo :

```{r, collapse = TRUE}
sim_jaccard_result$SJ_Perms<-c(mean(firmas_perms$doc_1 == firmas_perms$doc_2),
                                        mean(firmas_perms$doc_1 == firmas_perms$doc_3),
                                        mean(firmas_perms$doc_1 == firmas_perms$doc_4),
                                        mean(firmas_perms$doc_2 == firmas_perms$doc_3),
                                        mean(firmas_perms$doc_2 == firmas_perms$doc_4),
                                        mean(firmas_perms$doc_3 == firmas_perms$doc_4))

# Comparación de resultados. 
DT::datatable(sim_jaccard_result%>%
    mutate_if(is.numeric, funs(round(., 3))))
```

Con tan sólo 12 permutaciones tenemos una muy buena aproximación al verdadero valor de la similitud de Jaccard. 


Ahora veamos qué sucede si repetimos varias veces:

```{r, collapse = TRUE}
# Fijamos el no. de permutaciones a usar. 
num_hashes <- 12

# Repetimos el experimento 50 veces. 
firmas_rep <- map(1:50, function(i){
    #En cada iteración:
  
    # Se obtienen num_hashes permutaciones. 
    perms <- map(1:num_hashes, sample, x = 1:nrow(df), size = nrow(df)) 
    
    # Se calcula la matriz de firmas 
    df_out <- calc_firmas_perm(df, perms)
    
    # Se indica el no. de iteración. 
    df_out$rep <- i
    
    # La salida es la matriz de firmas. 
    df_out
})

# Calcula proporción de coincidencias en matriz de firmas para cada par de documentos. 
# Extrae cuantiles 10%, 50% y 90% de las 50 simulaciones

# Doc.1 vs Doc.2
map_dbl(firmas_rep, ~ mean(.x$doc_1 == .x$doc_2))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(3)
# Doc.1 vs Doc.3
map_dbl(firmas_rep, ~ mean(.x$doc_1 == .x$doc_3))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(3)
# Doc.1 vs Doc.4
map_dbl(firmas_rep, ~ mean(.x$doc_1 == .x$doc_4))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(3)
# Doc.2 vs Doc.3
map_dbl(firmas_rep, ~ mean(.x$doc_2 == .x$doc_3))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(3)
# Doc.2 vs Doc.4
map_dbl(firmas_rep, ~ mean(.x$doc_2 == .x$doc_4))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(3)
# Doc.3 vs Doc.4
map_dbl(firmas_rep, ~ mean(.x$doc_3 == .x$doc_4))  %>% 
    quantile(c(0.1, 0.5, 0.9)) %>% round(3)
```

Que indica que nuestro procedimiento da estimaciones razonables
de las similitudes de Jaccard.

*Observación*: si la similitud de dos documentos es cero, entonces
este procedimiento siempre da la respuesta exacta. ¿Por qué? Porqué si la similitud entre dos documentos es cero, entonces no hay coliciones (coincidencias) en las columnass de la matriz de firmas. 

---

** Demostración del resultado**

Ahora damos un argumento de este resultado.
Consideremos dos columnas $a,b$ de la matriz
de 0's y 1's, con conjuntos de tejas asociados $A,B$.

- Permutamos los reglones de las dos columnas $a$ y $b$.
- Sea $k$ la posición donde aparece el primer $(0,1)$, $(1,0)$ o $(1,1)$.
- Hay tantos renglones $(1,1)$ como elementos en $A\cap B$. Y hay tantos
renglones  $(0,1)$, $(1,0)$ o $(1,1)$ como elementos en $A\cup B$.
- Todos estos $|A\cup B|$ reglones tienen la misma probabilidad de aparecer
en la posición $k$.
- Entonces, la probabilidad condicional de que el renglón $k$ sea de tipo $(1,1)$, dado que es de algún tipo de $(1,0), (0,1), (1,1)$, es 
$$\frac{|A\cap B|}{|A\cup B|},$$
que es la similitud de Jaccard de los dos documentos.


## Algoritmo para calcular la matriz de firmas

El primer problema con el procedimiento de arriba es el costo de calcular las permutaciones y permutar la matriz característica (tejas-documentos). 
Generalmente no queremos hacer esto, pues el número de tejas es grande.

Escribimos un algoritmo para hacer el cálculo de la matriz
de firmas dado que
tenemos las permutaciones, sin permutar la matriz y recorriendo
por renglones. 

Supongamos que tenemos $h_1,\ldots, h_k$ permutaciones. Denotamos por $SIG_{i,c}$ el elemento de la matriz de
firmas para la $i$-ésima permutación y el documento $c$.


```{block2, type='resumen'}
**Cálculo de matriz de firmas**

  Inicializamos la matriz de firmas como $SIG_{i,c}=\infty$. Para cada
renglón $r$ de la matriz original:

  - Para cada columna $c$:
      1. Si $c$ tiene un cero en el renglón $r$, no hacemos nada.
      2. Si $c$ tiene un uno en el renglón $r$, ponemos para cada $i$
            $$SIG_{i,c} = \min\{SIG_{i,c}, h_i(r)\}.$$
```

#### Ejercicio {-}
Aplicar este algoritmo al ejercicio \@ref(ej1).


**Ver mis notas a mano para la solución del ejercicio.**


---

### Ejemplo {-}

Consideramos el ejemplo que vimos y hacemos una implementación simple del algoritmo
de arriba:

```{r}
# Convertimo sa dataframe la matriz de tejas por documento. 
df <- as.data.frame(df)

# Convierte las tejas a nombre de filas y luego el dataframe a matriz. 
mat_df <- df %>% column_to_rownames('tejas') %>% as.matrix

# Función para calcular firmas con el algoritmo. 
calc_firmas <- function(mat_df, permutaciones){
    
    # Define el no. de permutaciones. 
    num_hashes <- length(permutaciones)
    
    # Inicializa el algoritmo con matriz de infinitos. 
    # No de filas = no permutciones, no. de columnas = no. documentos. 
    firmas <- matrix(Inf, ncol = ncol(mat_df), nrow = num_hashes)
    
    # Para cada renglón de la matriz de tejas: 
    for(r in 1:nrow(mat_df)){
        
        # Extrae los índices de las columnas que tienen unos. 
        indices <- mat_df[r, ] > 0
        
        # Selecciona el mínimo (elemento a elemento) entre el valor de la matriz de firmas y 
        # El valor r en cada permutación
        firmas[, indices] <- pmin(firmas[, indices], map_int(permutaciones, r))
    }
    
    # Devuelve las firmas. 
    firmas
}

t2<-Sys.time()
# Calcula la matriz de firmas para las permutaciones definidas anteriormente 
firmas_2 <- calc_firmas(mat_df, permutaciones)%>%
  as_data_frame
t2<-Sys.time()-t2

# Asignamos nombres de filas. 
colnames(firmas_2)<-paste("doc",1:4,sep="_")

# Matriz de firmas
firmas_2
```

```{r}
# calculamos proporción de coliciones para el algoritmo descrito. 
sim_jaccard_result$SJ_Algoritmo<-c(mean(firmas_2$doc_1 == firmas_2$doc_2),
                                        mean(firmas_2$doc_1 == firmas_2$doc_3),
                                        mean(firmas_2$doc_1 == firmas_2$doc_4),
                                        mean(firmas_2$doc_2 == firmas_2$doc_3),
                                        mean(firmas_2$doc_2 == firmas_2$doc_4),
                                        mean(firmas_2$doc_3 == firmas_2$doc_4))

# Comparación de resultados. 
DT::datatable(sim_jaccard_result%>%
    mutate_if(is.numeric, funs(round(., 3))))
```


Obtenemos la misma matriz de firmas y las mismas estimaciones para la similitud de Jaccard que cuando hicimos el cálculo de las matrices permutadas. Pero el tiempo de máquina usado bajo este algoritmo, aún para este ejemplo tan sencillo, es menor:

```{r}
# Tiempo de máquina generando las permutaciones
t1
# Tiempo de máquina con algoritmo. 
t2
```


## Funciones hash

El siguiente defecto que tiene nuestro algoritmo hasta ahora
es que es necesario simular
y almacenar las distintas permutaciones (quizá usamos cientos, para estimar
con más precisión las similitudes) que vamos a utilizar. Estas permutaciones
son relativamente grandes y quizá podemos encontrar una manera más rápida de "simular"
las permutaciones.


### Ejercicio{-}
En nuestro ejemplo anterior, tenemos $107$ tejas. Consideramos una funciones de
la forma (como se sugiere en [@mmd]):

$$h(x) = (ax + b) \bmod 107$$
donde $x$ es el índice de los renglones, escogemos $a$ al azar entre 1 y 106, y $b$ se escoge al azar
entre 0 y 106. ¿Por qué es apropiada una función de este tipo?


- Demuestra primero que
la función $h(x)$ es una permutación de los enteros $\{ 0,1,\ldots, 106 \}$. Usa el
hecho de que $107$ es un número primo. 
- Si escogemos $a, b$ al azar, podemos generar distintas permutaciones.
- Esta familia de funciones no dan todas las posibles permutaciones, pero
pueden ser suficientes para nuestros propósitos, como veremos más adelante.

**Observación**: si $p$ no es primo, nuestra familia tiene el defecto de que
algunas funciones pueden nos ser permutaciones. Por ejemplo,
si 
$$h(x) = 4x + 1 \bmod 12,$$
entonces $h$ mapea el rango $\{0,1,\ldots, 11\}$ a
```{r}
h <- function(x){  (4*x +1) %% 12}
h(0:11)
```

Más de un elemento en el vector (0, 1, 2,.., 11) es mapeado al mismo número. Esto no es útil cuando buscamos permutaciones de renglones. 

---

### Ejemplo {-}

Vamos a resolver nuestro problema simple usando funciones hash como las del ejercicio anterior

```{r}
# Extraemos el no. de renglones
num_renglones <- nrow(mat_df)

# Función hash para obtener permutaciones. 
hash_simple <- function(primo){
  
  # Fijamos el valor de a y b de forma aleatoria. 
  # a debe estar entre 0 y primo-1
  # b debe estar entre 1 y primo-1  
  a <- sample.int(primo - 1, 2)
  
  # Creamos la función hash dependiendo del primo y los valores a y b
  hash_fun <- function(x) {
        # restamos y sumamos uno para mapear a enteros positivos
        ((a[1]*(x-1) + a[2]) %% primo) + 1
  }
  
  # Devuelve la función hash. 
  hash_fun
}

# Fija la semilla. 
set.seed(132)

# Obtenemos dos funciones hash distintas y aleatorias 
hash_f <- map(1:2, ~ hash_simple(primo = 107))
```

Podemos examinar algunas de estas funciones:
```{r}
# Evaluamos las funciones hash en los renglones 1 a 107 obteniendo una permutación. 
hash_f[[1]](1:107)

# Veamos que no existan duplicados. 
hash_f[[1]](1:107) %>% duplicated %>% any
hash_f[[2]](1:107) %>% duplicated %>% any
```

De esta manera, ya no estaría almacenando las 12 permutaciones, sino 12 funciones hash que evalúo para cada renglón cuando implemente el algoritmo de las firmas. 


Reescribimos nuestra función *calc_firmas* para usar las funciones
hash en lugar de permutaciones:

```{r}
# Función para calcular firmas de documentos con funciones hash en lugar de permutaciones. 
calc_firmas_hash <- function(mat_df, hash_f){
  
    # Extrae el no. de permutaciones
    num_hashes <- length(hash_f)
    
    # Inicializa la matriz de firmas en infinitos. 
    # No. de filas = no. permutaciones (hashes), no. de columnas = no. documentos. 
    firmas <- matrix(Inf, ncol = ncol(mat_df), nrow = num_hashes)
    
    # Para cada fila de la matriz de tejas: 
    for(r in 1:nrow(mat_df)){
        # Extrae las columnas distintas de cero. 
        indices <- mat_df[r, ] > 0
        
        # Selecciona el mínimo (elemento a elemento) entre el valor de la matriz de firmas y 
        # El valor de la función hash en r
        firmas[, indices] <- pmin(firmas[, indices], map_dbl(hash_f, ~.(r)))
    }
    
    # Devuelve las firmas. 
    firmas
}
```


```{r}
# Fijamos la semilla.
set.seed(2851)

# Creamos 12 funciones hash al azar. 
# Nota: se usa 107 poruqe el no. de tejass distintas en este caso (107) es primo. 
# Si no fuera primo, se tendría que usar el primo más cercano que sea mayor a no. de tejas. 
hash_f <- map(1:12, ~ hash_simple(primo = 107))

# Calcula las firmass usando la función con hashes

firmas_2 <- calc_firmas_hash(mat_df, hash_f)%>% as_data_frame()

colnames(firmas_2)<-paste("doc",1:4,sep="_")

# Obtiene la matriz de firmas. 
firmas_2

```
```{r}
# Calculamos proporción de coliciones para el método que usa funciones hash. 
sim_jaccard_result$SJ_FunHash<-c(mean(firmas_2$doc_1 == firmas_2$doc_2),
                                        mean(firmas_2$doc_1 == firmas_2$doc_3),
                                        mean(firmas_2$doc_1 == firmas_2$doc_4),
                                        mean(firmas_2$doc_2 == firmas_2$doc_3),
                                        mean(firmas_2$doc_2 == firmas_2$doc_4),
                                        mean(firmas_2$doc_3 == firmas_2$doc_4))

# Comparación de resultados. 
DT::datatable(sim_jaccard_result%>%
    mutate_if(is.numeric, funs(round(., 3))))
```

La matriz de firmas evidente ya no es la misma que en los casos anteriores. Además, la aproximación con pocas funciones hash no es tan buena, pero cuando se tiene un gran número de tejas, la mejora en tiempo de ejecución sí lo es. 


Cuando el número de tejas no es exactamente un primo, hacemos lo siguiente:

```{block2, type = 'resumen'}
**Hash de tejas numeradas**
    
 Supongamos que tenemos $\{0,1,\ldots, m-1\}$ tejas numeradas. Seleccionamos un
primo $p\geq m$, y definimos las funciones
$$H =\{ h_{a,b}(x) = (ax + b) \bmod p \}$$
En lugar de escoger una permutación al azar, escogemos
$a \in \{1,\cdots, p-1\}$, y $b \in \{0,\cdots, p-1\}$ al azar. Usamos
en el algoritmo estas funciones $h_i$ como si fueran permutaciones.
```

**Observación**: El único detalle que hay que observar aquí es que los valores hash o cubetas
a donde se mapean las tejas ya no están en el rango $\{0,1,\ldots, m-1\}$ , de manera
que no los podemos interpretar como permutaciones. Pero pensando un poco, vemos que
no hay ninguna razón para interpretar los valores
de $h_i(r)$ como renglones de una matriz permutada, y no necesariamente
$h_i$ tiene que ser una permutación de los renglones. $h_i$ puede ser una
función que mapea renglones (tejas) a un rango grande de enteros, de forma
que la probabilidad de que distintos renglones sean mapeados a un mismo hash 
sea muy baja.


Obtener el minhash es simplemente encontrar el mínimo entero de los valores hash
que corresponden a tejas que aparecen en cada columna.


### Ejemplo {-}

```{r}
# Fija la semilla
set.seed(28511)

# Obtiene las 12 funciones hash usando 883 como primo
hash_f <- map(1:12, ~ hash_simple(primo = 883))

# calcula la matriz de firmas. 
firmas <- calc_firmas_hash(mat_df, hash_f)%>%as_data_frame()

colnames(firmas)<-paste("doc",1:4,sep="_")

# Matriz de firmas 
firmas

```

```{r}
# Calculamos proporción de coliciones bajo una función hash con otro nùmero primo. 
sim_jaccard_result$SJ_NoPrimo<-c(mean(firmas$doc_1 == firmas$doc_2),
                                        mean(firmas$doc_1 == firmas$doc_3),
                                        mean(firmas$doc_1 == firmas$doc_4),
                                        mean(firmas$doc_2 == firmas$doc_3),
                                        mean(firmas$doc_2 == firmas$doc_4),
                                        mean(firmas$doc_3 == firmas$doc_4))

# Comparación de resultados. 
DT::datatable(sim_jaccard_result%>%
    mutate_if(is.numeric, funs(round(., 3))))
```

### Funciones hash: discusión {-}

En consecuencia, como solución general para nuestro problema de minhashing podemos seleccionar un primo $p$ muy grande y utilizar la familia de congruencias $(ax+b)\bmod p$, seleccionando
$a$ y $b$ al azar (ver [implementación de Spark](https://github.com/apache/spark/blob/v2.1.0/mllib/src/main/scala/org/apache/spark/ml/feature/MinHashLSH.scala), donde se utiliza un primo fijo MinHashLSH.HASH_PRIME) .

Otro enfoque es hacer hash directamente de las tejas (caracteres).
En este caso, buscamos una función
hash de cadenas a enteros grandes que "revuelva" las cadenas a un rango
grande de enteros. 
Es importante la calidad de la función hash, pues no queremos tener demasiadas
colisiones aún cuando existan patrones en nuestras tejas.

Por ejemplo,
podemos utilizar la función *hash_string* del paquete textreuse [@R-textreuse] (implementada
en C++):

```{r, collapse = TRUE}
# Ejemplos de funciones has para cadenas de caracteres. 
textreuse::hash_string('a')
textreuse::hash_string('b')
textreuse::hash_string('El perro persigue al gato') 
textreuse::hash_string('El perro persigue al gat') 
``` 

Para obtener otras funciones hash, podemos usar una técnica distinta. Escogemos
al azar un entero, y hacemos bitwise xor con este entero al azar. 
En la implementación de *textreuse*, por ejemplo, se hace:

```{r}
# Se fija la semilla 
set.seed(123)

# Función para generar un hash
generar_hash <- function(){
    # Genera un aleatorio entre -2147483648 y 2147483648
    r <- as.integer(stats::runif(1, -2147483648, 2147483647))
    
    # Genera el hash con bitwise xor
    funcion_hash <- function(x){
        bitwXor(textreuse::hash_string(x), r)    
    }
    
    # Devuelve la función hash
    funcion_hash
}

# Se generan dos funciones hash disstintas
h_1 <- generar_hash()
h_2 <- generar_hash()

# Se evalua el mismo caracter en los dos hashes. 
h_1("abcdef")
h_2("abcdef")
```

Otra opción es utilizar como función básica otra función hash probada, como *murmur32*
o *xxhash32* (la segunda es más rápida que la primera), que también pueden
recibir semillas para obtener distintas funciones:

```{r}
# Fija la semilla
set.seed(123)

# Funcion para generar hashes con xxhash32
generar_xxhash <- function(){
    # Genera un entero aleatorio entre 1 y 2147483647 
    r <- as.integer(stats::runif(1, 1, 2147483647))
    
    # Obtiene los hashes para cada teja. 
    funcion_hash <- function(shingles){
        hash_hex <- 
          map_chr(shingles, 
            ~ digest::digest(.x, algo = "xxhash32", seed = r, serialize = FALSE))
        # convertir a entero, evitar overflow
        strtoi(substr(hash_hex, 2, 8), 16)
    }
    funcion_hash
}

# Genera dos funciones hash distintas
h_1 <- generar_xxhash()
h_2 <- generar_xxhash()

# Evalua los hashes en el mismo texto
h_1("abcdef")
h_2("abcdef")
```

### Ejemplo {-}
Usando estas funciones hash, nuestro miniejemplo se vería como sigue:

```{r}
# Funcóin para calcular matriz de tejas con hashes generados por xxhash32
calc_firmas_hash <- function(mat_df, hash_funs){
    # Obtiene no. de hashes
    num_hashes <- length(hash_funs)
    
    # obtiene las tejas 
    tejas <- rownames(mat_df)
    
    # Inicializa la matriz de firmas. 
    firmas <- matrix(Inf, ncol = ncol(mat_df), nrow = num_hashes)
    
    # Para cada teja 
    for(r in 1:nrow(mat_df)){
        # Extrae las columnas que tienen 1's
        indices <- mat_df[r, ] > 0
        
        ## se calcula los hashes de cadenas
        hashes <- map_dbl(hash_funs, ~.(tejas[r]))
        
        ## Actualiar matriz de firmas
        firmas[, indices] <- pmin(firmas[, indices], hashes)
    }
    firmas
}

# Fija la semilla
set.seed(211)

# Obtiene 30 funciones hash. 
hash_f <- map(1:30, ~ generar_xxhash())

# Calcula la matriz de firmas
firmas <- calc_firmas_hash(mat_df, hash_f)%>%as_data_frame()

colnames(firmas)<-paste("doc",1:4,sep="_")


# Matriz de firmas . 
firmas
```

Y las similitudes son:

```{r}
# Calculamos proporción de coliciones bajo una función hash con otro nùmero primo. 
sim_jaccard_result$SJ_Texto<-c(mean(firmas$doc_1 == firmas$doc_2),
                                        mean(firmas$doc_1 == firmas$doc_3),
                                        mean(firmas$doc_1 == firmas$doc_4),
                                        mean(firmas$doc_2 == firmas$doc_3),
                                        mean(firmas$doc_2 == firmas$doc_4),
                                        mean(firmas$doc_3 == firmas$doc_4))

# Comparación de resultados. 
DT::datatable(sim_jaccard_result%>%
    mutate_if(is.numeric, funs(round(., 3))))
```



## Minhashing

Ahora podemos proponer una implementación para minhashing, utilizando el
algoritmo mostrado arriba. 

Para poder procesar los datos por renglón,
primero necesitamos organizar los datos por teja (originalmente están por documento, 
o columna), por ejemplo:

```{r}
# Extrae los valores únicos de las tejas para cada documento
tejas <- shingle_chars(textos, k = 4) %>% map(unique)

# Crea un data frame con tejas en las filas y documentos en las columnas. 
tejas_df <- data_frame(texto_id = 1:4, shingles = tejas) %>% # Data frame con no. texto y vector de tejas 
        unnest %>% # Se convierte a data frame
        mutate(shingle_n = as.numeric(factor(shingles))) %>% # Se asigna un id a cada teja distinta. 
        group_by(shingle_n) %>% select(-shingles) %>% # Se agrupa por id de teja
        summarise(textos = list(texto_id)) # se ingresan los documentos en los que aparece cada teja

# Matriz de tejas 
tejas_df
```

Construimos una función para invertir:

```{r}
# Función para tener tejas por renglón 
crear_tejas_reng <- function(textos, k = 4){
  
    # No de documentos 
    num_docs <- length(textos)
    
    # crear tejas únicas por texto.
    tejas <- shingle_chars(textos, k = k) %>% map(unique)
    
    # invertir
    tejas_df <- data_frame(texto_id = 1:num_docs, shingles = tejas) %>% # Data frame con no. texto y vector de tejas 
        unnest %>% # Se convierte a data frame
        mutate(shingle_n = as.numeric(factor(shingles))) %>% # Se asigna un id a cada teja distinta. 
        group_by(shingle_n) %>% select(-shingles) %>% # Se agrupa por id de teja
        summarise(textos = list(texto_id)) # Se ingresan los doc en los que aparece cada teja
    
    # Se hace una lista con los textos que contienen a cada teja, cuantos textos son y el tamaño. 
    list(tejas = tejas_df$textos, num_docs = num_docs, k = k)
}
```

Ya convertidos los tejas a números podemos usar las funciones hash modulares:

```{r}
# Genera función hahs modular 
generar_hash_mod <- function(p = 2038074743){
    # Obtiene valores aleatorios para a y b
    a <- sample.int(p - 1, 2)
    
    # Calcula la función hash con los valores específicos
    hash_fun <- function(x) {
    
        # restamos y sumamos uno para mapear a enteros positivos
        ((a[1]*(x - 1) + a[2]) %% p) + 1
    }
    # Devuelve la función hash
    hash_fun
}
```

Y finalmente reescribimos el algoritmo:

```{r}
# Algoritmo para calcular la matriz de firmas
calc_firmas_hash_reng <- function(tejas_obj, hash_funs){
  
    # No de documentos
    num_docs <- tejas_obj$num_docs
    
    # min hashing
    # Se fija el no. de hashes
    num_hashes <- length(hash_funs)
    
    # Se obtienen las tejas 
    tejas <- tejas_obj$tejas
    
    # Se inicializa la matriz de firmas
    # filas = no hashes, col= no doc. 
    firmas <- matrix(Inf, ncol = num_docs, nrow = num_hashes)
    
    # Para cada teja
    for(r in 1:length(tejas)){
      
        # calcular hashes de teja
        hashes <- map_dbl(hash_funs, ~.x(r))
        
        # extaer documentos que contienen la teja
        indices <- tejas[[r]]
        
        # actualizar matriz
        firmas[, indices] <- pmin(firmas[, indices], hashes)
    }
    
    # Matriz de firmas 
    firmas
}
```


```{r}

# Se fija la semilla 
set.seed(21121)

# Se generan 500 funciones hash.
hash_f <- map(1:500, ~ generar_hash_mod())

# Se crean las tejas para los documentos de ejemplo 
tejas_obj<- crear_tejas_reng(textos, k = 4) 

# Se calcula la matriz de firmas 
firmas <- calc_firmas_hash_reng(tejas_obj, hash_f)%>%
  as_data_frame()

colnames(firmas)<-paste("doc",1:4,sep="_")

DT::datatable(firmas)

```

```{r}
# Calculamos proporción de coliciones 
sim_jaccard_result$SJ_MinHash<-c(mean(firmas$doc_1 == firmas$doc_2),
                                        mean(firmas$doc_1 == firmas$doc_3),
                                        mean(firmas$doc_1 == firmas$doc_4),
                                        mean(firmas$doc_2 == firmas$doc_3),
                                        mean(firmas$doc_2 == firmas$doc_4),
                                        mean(firmas$doc_3 == firmas$doc_4))

# Comparación de resultados. 
DT::datatable(select(sim_jaccard_result,Sim_Jaccard,SJ_MinHash)%>%
    mutate_if(is.numeric, funs(round(., 3))))
```
Usando suficientes funciones minhash se tienen aproximaciones muy cercanas al verdadero valor de la similitud de Jaccard. 

## Ejemplo: tweets 

Ahora buscaremos tweets similares en una colección de un [concurso de
kaggle](https://www.kaggle.com/rgupta09/world-cup-2018-tweets/home?utm_medium=email&utm_source=mailchimp&utm_campaign=datanotes-20180823).

```{r, message=FALSE}
# Ruta de los datos
ruta <- "../datos/FIFA.csv"

# ruta google storage para descargar los datos. 
gc_ruta <- "https://storage.googleapis.com/fifa_tweets/FIFA.csv"

if(!file.exists(ruta)){
    # si el archivo no existe, descarga los datos. 
    download.file(gc_ruta, ruta)
} else {
    # si el archivo existe, carga los datoss.
    fifa <- read_csv("../datos/FIFA.csv")
}

# Extrae los tweets
tw <- fifa$Tweet

# Primeros 10 tweets
tw[1:10]

```

```{r algorenglon}
# Fija semilla 
set.seed(91922)

# Genera 50 funciones hash para texto
hash_f <- map(1:50, ~ generar_hash_mod())

# Extrae las tejas para los primeros 200 mil tweets y mide el tiempo
system.time(mat_tejas <- crear_tejas_reng(tw[1:200000], k = 5))

# Extrae la matriz de firmas para las tejas de los primeros 200 mil tweets y mide el tiempo
system.time(firmas <- calc_firmas_hash_reng(mat_tejas, hash_f))
```

La matriz de firmas tiene 50 renglones (una por hash) y 200 mil columnas (una por tweet).  Por ejemplo, ¿cuáles son tweets similares al primero?

```{r}
# Calculamos la proporción de coliciones de los documentos con el primer tweet. 
similitudes <- map_dbl(1:ncol(firmas), ~ mean(firmas[, .x] == firmas[, 1]))

# Extraemos el id de los tweets con al menos 40% de similitud
indices <- which(similitudes > 0.4)

# No. de tweets con al menos 40% de similitud
length(indices)

# Resumen de tweets similares ordenados por jaccard
similares <- data_frame(tweet = tw[indices],
           jacc_estimada = similitudes[indices]) %>%
    arrange(desc(jacc_estimada))

# Matriz de tweets similares al primero. 
DT::datatable(similares)
```

Aquellos con similitud de jaccard igual a 1 son tweets idénticos. En este sentido, podríamos encontrar el número de tweets repetidos. El primer tweet diferente tiene índice 434 (ordenando por Similitud de Jaccard) es decir, hay 433 tweets iguales al primero. 

## Minhash: algoritmo por documento

En muchas implementaciones, se usa un algoritmo por documento, en lugar de por teja:

- El algoritmo recorre documento por documento.
- Se calculan las tejas del documento.
- Para cada función hash: se aplica la función a todas las tejas y 
se toma el mínimo.
- Estos mínimos (tantos como funciones hash haya) dan la firma del documento.

Tiene la ventaja de no requerir preprocesamiento para hacer el índice de tejas,
pues típicamente los datos están organizados por documentos. Para datos distribuidos grandes,
el preprocesamiento necesario para aplicar el algoritmo por teja puede ser costoso.
La desventaja es
que recalculamos muchas veces la misma función hash para cada teja. Esta también
es la implementación que se utiliza en [Spark](https://github.com/apache/spark/blob/v2.1.0/mllib/src/main/scala/org/apache/spark/ml/feature/MinHashLSH.scala).

Podríamos hacer una implementación como sigue:

```{r}
# Función para crear tejas por documento. 
# las tejas serán convertidas a enteros
crear_tejas_doc <- function(textos, k = 4){
  
    # Se extrae el no de documentos 
    num_docs <- length(textos)
    
    # crear tejas únicas por documento 
    tejas <- shingle_chars(textos, k = k) %>% map(unique)
    
    # Dataframe con documento y tejass
    tejas_df <- data_frame(texto_id = 1:num_docs, shingles = tejas) %>%
        unnest %>% #se conviert a data frame
        mutate(shingle_n = as.numeric(factor(shingles))) %>% # las tejas serán convertidas a enteros
        group_by(texto_id) %>% # se agrupa por documento 
        summarise(shingles = list(shingle_n)) # se hace una lista de las tejas 
    
    # Lista de salida con tejas, no de doc. y tamaño de tejas 
    list(tejas = tejas_df$shingles, num_docs = num_docs, k = k)
}
```

Y obtenemos las tejas ordenadas por documento:

```{r}
# Se crean las tejas por documento 
tejas_obj <- crear_tejas_doc(textos, k = 4)

# Veamos la salida de las tejas 
tejas_obj$tejas
```


Nuestra función ahora procesa documento por documento:


```{r}
# Función para calcular la matriz de firmas por documento. 
calc_firmas_hash_doc <- function(tejas_obj, hash_funs){
    # Se extrae el no. de documento
    num_docs <- tejas_obj$num_docs
    
    # Se fija el no. de hashes
    num_hashes <- length(hash_funs)
    
    # Se extraen las tejas 
    tejas <- tejas_obj$tejas
    
    # Se hace una lista de tamaño igual a numero de documentos 
    firmas <- vector("list", num_docs)
    
    # Para cada documento: 
    for(i in 1:num_docs){
        # Se calcula la firma para cada teja 
        firmas[[i]] = map_dbl(hash_f, ~ min(.x(tejas[[i]])))
    }
    
    # Se crea un data frame 
    data_frame(doc_id = 1:num_docs, firma = firmas)
}
```

```{r}

# Se fija la semilla 
set.seed(28511)

# Se crean 12 hashes 
hash_f <- map(1:12, ~ generar_hash_mod())

# Se crean las tejas por documento
tejas_obj <- crear_tejas_doc(textos, k = 4)

# Se calculan las firmas por documento 
firmas_df <- calc_firmas_hash_doc(tejas_obj, hash_f)

# Lista de firmas 
firmas_df

# Se calcula la proporción de coliciones. 
mean(firmas_df$firma[[1]]==firmas_df$firma[[2]])
mean(firmas_df$firma[[1]]==firmas_df$firma[[3]])
mean(firmas_df$firma[[3]]==firmas_df$firma[[4]])
```

Otra opción es usar una implementación donde nos ahorramos
numerar las tejas, pero pagamos el costo de una función de hash
más complicada para cadenas:

```{r}
# Función para calcular matriz de firmass por documento con tejas en texto
calc_firmas_hash_doc_str <- function(tejas, hash_funs){
    
    # no de documentos. 
    num_docs <- tejas_obj$num_docs
    
    # no. de hashes
    num_hashes <- length(hash_funs)
    
    # Lista de firmas por documento. 
    firmas <-vector("list", num_docs)
    
    # Para cada documento 
    for(i in 1:num_docs){
        
      # Se extrae las firmas para tejas con texto
        firmas[[i]] = map_dbl(hash_funs, ~ min(.x(tejas[[i]])))
    }
    
    # matriz de firmas 
    firmas
}
```


```{r}

# Fija la semilla 
set.seed(2)

# Genera 12 funciones hash que reciben texto y no números
hash_f <- map(1:12, ~ generar_xxhash())

# Genera las tejas para los textos 
tejas <- shingle_chars(textos, k = 4)

# Calcula la matriz de firmass 
firmas <- calc_firmas_hash_doc_str(tejas, hash_f)

# Proporción de coliciones por documento 
mean(firmas[[1]]==firmas[[2]])
mean(firmas[[1]]==firmas[[3]])
mean(firmas[[3]]==firmas[[4]])
```


## Ejemplo: tweets, usando textreuse

En el paquete *textreuse*, se usa un algoritmo por documento, y hace
hash directamente de las cadenas de las tejas

```{r}
# Cargar librería
library(textreuse)

# Genera minhashes
minhash <- minhash_generator(50)
```

```{r textreuse}
# este caso ponemos en hash_func los minhashes, para después
# usar la función pairwise_compare (que usa los hashes)
system.time(
corpus_tweets <- TextReuseCorpus(text = tw[1:100000], # 100 mil tweetsss
    tokenizer = shingle_chars, k = 5, lowercase = FALSE, # tejas de tamaño 5
    hash_func = minhash, keep_tokens = TRUE, # minhas generados
    keep_text = TRUE, skip_short = FALSE)
)
```

Busquemos tweets similares a uno en particular

```{r}
# Primer tweet 
corpus_tweets[[1]]$content

# Firmas
min_hashes <- hashes(corpus_tweets)

# Calculo de la similitud 
similitud <- map_dbl(min_hashes, ~ mean(min_hashes[[1]] == .x))

# Tweets con silitud mayor a 40% con el primer tweet
indices <- which(similitud > 0.4)

# No de tweetss similares al 40%
length(names(indices))
```

```{r}
# Veamos los primeros 5 documentos con similitud mayor a 40%
names(indices)[1:5] 

# Valor de la similitud 
similitud[indices][1:5] 

# Imprimimos tweets sismilares 
map(names(indices), ~ corpus_tweets[[.x]]$content)[1:5]
```


¿Cuáles son las verdaderas distancias de Jaccard? Por ejemplo,

```{r}
# Distancia de Jaccard entre doc 1 y doc 417
jaccard_similarity(
  shingle_chars(corpus_tweets[["doc-1"]]$content, lowercase=TRUE, k = 5),
  shingle_chars(corpus_tweets[["doc-417"]]$content, lowercase=TRUE, k = 5)
  )
```

```{r}

# Distancia de Jaccard entre doc 1 y doc 5
jaccard_similarity(
  shingle_chars(corpus_tweets[["doc-1"]]$content, lowercase=TRUE, k = 5),
  shingle_chars(corpus_tweets[["doc-5"]]$content, lowercase=TRUE, k = 5)
  )
```

**Observación**: Una vez que calculamos los que tienen similitud
aproximada $>$ $0.4$, podemos calcular la función de Jaccard exacta
para los elementos similares resultantes.


## Buscando vecinos cercanos

Aunque hemos reducido el trabajo para hacer comparaciones de documentos,
no hemos hecho mucho avance en encontrar todos los pares similares
de la colección completa de documentos. Intentar calcular similitud
para todos los pares (del orden $n^2$) es demasiado trabajo:

```{r simstextreuse}
# Todas la comparaciones por pares para los primeros 200 tweets
system.time(
pares  <- pairwise_compare(corpus_tweets[1:200], ratio_of_matches) %>%
      pairwise_candidates()
)

# Filtramos aquellos con similitud mayor a 20%
pares <- pares %>% filter(score > 0.20) %>% arrange(desc(score)) 
nrow(pares)
```

Sólo para los primeros 200 tweets tenemos 19,900 comparaciones. Nótese que si tuviéramos $10$ veces más tweets (una fracción todavía del conjunto completo) el número de comparaciones se multiplica por $100$ aproximadamente (es decir 1,990,000) y por lo tanto también el tiempo de procesamiento.


En la siguiente parte veremos como aprovechar estos minhashes para hacer una
búsqueda eficiente de pares similares.

## Locality Sensitive Hashing (LSH) para documentos

Calcular todas las posibles similitudes de una
colección de un conjunto no tan grande de documentos es difícil. Sin
embargo, muchas veces lo que nos interesa es simplemente agrupar
colecciones de documentos que tienen alta similitud (por ejemplo para
deduplicar, hacer clusters de usuarios muy similares, etc.).

Una técnica para encontrar vecinos cercanos de este tipo es Locality Sensitive
Hashing (LSH), que generalizamos en la sección siguiente. Comenzamos
construyendo LSH basado en las firmas de minhash. La idea general
es: 

```{block2, type = 'resumen'}
**Idea general de Minshashing LSH**

- Recorremos la matriz de firmas documento por documento
- Asignamos el documento a una cubeta dependiendo de sus valores minhash (su firma).
- Todos los pares de documentos que caen en una misma cubeta son candidatos a pares similares. Generalmente tenemos mucho menos candidatos que el total de posibles pares.
- Checamos todos los pares candidatos dentro de cada cubeta que contenga más de un
elemento (calculando similitud de Jaccard original)
```

Nótese que con este método hemos resuelto de manera aproximada nuestro problema 
de encontrar pares similares. En la siguiente parte discutiremos cómo decidir
si es o no una buena aproximación. Veremos también formas de diseñar las cubetas para obtener candidatos con la
similitud que busquemos (por ejemplo, mayor a $0.5$, mayor a $0.9$, etc.). 


Antes veremos algunas posibilidades construidas a mano para lograr
nuestro objetivo. Usaremos en estos la implementación del algoritmo por documento.

### Ejemplo: todos los minshashes coinciden

Calculamos los minhashes, y creamos una cubeta para cada minhash diferente. Los
candidatos a similitud son pares que caen en una misma cubeta. Como usamos todos
los hashes, los candidatos tienden a ser textos muy similares.

Usamos un primo chico para leer los resultados más fácilmente:

```{r}
# Creamos textos duplicados para el ejemplo
textos_dup <- c(textos,  textos[2], "este es el Documento de Ejemplo" ,
                paste0(textos[2], '!'), 'texto diferente a todos')

# Imprime los textos
textos_dup

# Fija la semilla 
set.seed(21)

# Genera 12 funciones hash 
hash_f <- map(1:12, ~ generar_hash_mod(p = 107))

# Genera las tejas por documento 
tejas <- crear_tejas_doc(textos_dup, k = 4)

# Calcula la matriz de firmas por documento
firmas <- calc_firmas_hash_doc(tejas, hash_f)

# Cubetiza las firmas, pegando la firma de cada documento con guiones. 
firmas_2 <- firmas %>% 
    mutate(cubeta = map_chr(firma, paste, collapse ="-")) %>% select(-firma)

# Firmas en cubetas
firmas_2
```

Ahora agrupamos por cubetas:

```{r}
# Se agrupa por cubetas y se listan los documentos en cada cubeta
cubetas_df <- firmas_2 %>% group_by(cubeta) %>% 
    summarise(docs = list(doc_id)) %>% 
    mutate(n_docs = map_int(docs, length)) 
```

Por ejemplo, hay 3 documentos en la cubeta 4-1-2-6-1-2-7-2-1-1-3-2. Es decir, son documentos idénticos. 


y filtramos las cubetas con más de un elemento:

```{r}
# Filtra cubetas con más de un documento
candidatos <- cubetas_df %>% filter(n_docs > 1)

# Ver candidatos
candidatos
```

Los candidatos de alta similud se extraen de estas cubetas
que tienen más de $1$ elemento. Son:

```{r}
# vemos los documentos candidatos a ser similares
candidatos$docs

textos_dup[candidatos$docs[[1]]]
```

Ahora podemos extraer los pares de similitud alta:

```{r}
# QUE HACE ESTA FUNCION????

# Función para extraer pares de similitud alta
extraer_pares <- function(candidatos, cubeta, docs, textos = NULL){
  
   enq_cubeta <- enquo(cubeta) 
   enq_docs <- enquo(docs) 
   pares <- candidatos %>% 
    mutate(pares = map(!!enq_docs, ~ combn(sort(.x), 2, simplify = FALSE))) %>% # Genera pares de documentos de los listados en candidatos
    select(!!enq_cubeta, pares) %>% unnest %>% 
    mutate(a = map_int(pares, 1)) %>% 
    mutate(b = map_int(pares, 2)) %>% 
    select(-pares) %>% select(-!!enq_cubeta) %>% unique
   if(!is.null(textos)){
       pares <- pares %>% mutate(texto_a = textos[a], texto_b = textos[b])
   }
   pares
}

# Vemos pares de candidatos con alta similitud
DT::datatable(extraer_pares(candidatos, cubeta, docs, textos = textos_dup))
```

### Ejemplo: algún grupo de minhashes coincide

Si quisiéramos capturar como candidatos pares de documentos con similitud
más baja, podríamos pedir que coincidan solo algunos de los hashes. Por ejemplo,
para agrupar textos con algún grupo de $2$ minshashes iguales, podríamos hacer:



```{r}
# Partición de los hashes de dos en dos. 
particion <- split(1:12, ceiling(1:12 / 2))
particion

# Función para separar cubetas con base en partición
separar_cubetas_fun <- function(particion){
  
    function(firma){
              
        map_chr(particion, function(x){
            # Junta los elementos de la partición
            prefijo <- paste0(x, collapse = '')
            
            # Pega la firma con guiones
            cubeta <- paste(firma[x], collapse = "-")
            
            # Pega los elementos de la partición y la firma. 
            paste(c(prefijo, cubeta), collapse = '|')
        })
    }
}

# Función para separar cubetas con la partición definida. 
sep_cubetas <- separar_cubetas_fun(particion)

# Se aplica la separa ción de cubetas a la primer firma
sep_cubetas(firmas$firma[[1]])

# Firmas en cubetas por partición
firmas_3 <- firmas %>% 
    mutate(cubeta = map(firma, sep_cubetas)) %>% 
    select(-firma) %>% unnest

# Se ven lasfirmas por cubetas "parciales"
firmas_3
```

De esta manera, cada documento tiene 6 posibles cubetas (pedacitos de firmas) con los que vamos a buscar elementos similares. 

Ahora agrupamos por cubetas:

```{r}
# Se agurpa por cubeta y se listan los documentos en cada una. 
cubetas_df <- firmas_3 %>% group_by(cubeta) %>% 
    summarise(docs = list(doc_id)) %>% 
    mutate(n_docs = map_int(docs, length)) 
```

y filtramos las cubetas con más de un elemento:

```{r}
# Se filtran las cubetas con más de un elemento
candidatos <- cubetas_df %>% filter(n_docs > 1)
candidatos
```

Hay 7 cubetas (por pares) que tienen más de un elemento. Estos son los documentos candidatos a tener alta similitud. Aplicamos la función para extraer los pares de documentos similares: 
```{r}
DT::datatable(extraer_pares(candidatos, cubeta, docs, textos = textos_dup) %>% 
                  arrange(texto_a))
```

Está captando un par de textos similares que con las cubetas completas no aparecía. 


## Tarea {-}

1. Calcula la similitud de Jaccard de las cadenas "Este es el ejemplo 1" y "Este es el ejemplo 2", usando tejas de tamaño $3$.

2. (Ejercicio de [@mmd]) Considera la siguiente matriz de tejas-documentos:

```{r}
mat <- matrix(c(0,1,0,1,0,1,0,0,1,0,0,1,0,0,1,0,0,0,1,1,1,0,0,0),
              nrow = 6, byrow = TRUE)
colnames(mat) <- c('d_1','d_2','d_3','d_4')
rownames(mat) <- c(0,1,2,3,4,5)
mat
```

  - Sin permutar esta matriz, calcula la matriz de firmas minhash usando las siguientes funciones
  hash: $h_1(x) = 2x+1\mod 6$, $h_2(x) = 3x+2\mod 6$, $h_3(x)=5x+2\mod 6$.
Recuerda que $a\mod 6$ es el residuo que se obtiene al dividir a entre $6$, por ejemplo $14\mod 6 = 2$, y usa la numeración de renglones comenzando en $0$.
  - Compara tu resultado usando el algoritmo por renglón que vimos en clase,
    y usando el algoritmo por columna (el mínimo hash de los números de renglón que tienen un $1$).
  - ¿Cuál de estas funciones hash son verdaderas permutaciones?
  - ¿Qué tan cerca están las similitudes de Jaccard estimadas por minhash de las verdaderas similitudes?

3. Funciones hash. Como vimos en clase, podemos directamente hacer hash
de las tejas (que son cadenas de texto), en lugar de usar hashes de números enteros (número de renglón). Para lo siguiente, puedes usar la función *hash_string* del paquete *textreuse* (o usar la función  *pyhash.murmur3_32* de la librería *pyhash*):

 - Calcula valores hash de algunas cadenas como 'a', 'Este es el ejemplo 1', 'Este es el ejemplo 2'. 
 - Calcula los valores hash para las tejas de tamaño $3$ de 'Este es el ejemplo 1'. ¿Cuántos valores obtienes?
 - Usa los números del inciso anterior para calcular el valor minhash del texto anterior. 
 - Repite para la cadena 'Este es otro ejemplo.', y usa este par de minhashes para estimar la similitud de Jaccard (en general usamos más funciones minhash para tener una buena estimación, no solo una!).
- Repite los pasos anteriores para  $10$ funciones minhash (puedes usar *minhash_generator* de *textreuse*, o usar distintas semillas para *pyhash.murmur3_32*, o algunas de las funciones que generan funciones hash que vimos en clase).

4. Utiliza el código visto en clase para encontrar pares de similitud alta en la colección de tweets que vimos en clase. Utiliza unos $15$ hashes para encontrar tweets casi duplicados. ¿Cuántos tweets duplicados encontraste?
¿Qué pasa si usas menos o más funciones hash?
